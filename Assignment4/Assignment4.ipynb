{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"train\\train.csv\")\n",
    "test = pd.read_csv(r\"test\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "lb= LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, ..., 0, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train.text.values\n",
    "y = lb.fit_transform(train.author.values)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have used GloVE for vector representations for words. As the file size is above 5Gb I havent uploaded the file\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(\"C:\\\\Users\\\\ME\\\\Desktop\\\\ADS\\\\Project\\\\glove.840B.300d.txt\",'r',encoding='utf8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method isalpha() checks whether the string consists of alphabetic characters only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    words = [lemm.lemmatize(w) for w in words]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/15663 [00:00<?, ?it/s]\n",
      "  0%|                                                                             | 1/15663 [00:03<14:05:50,  3.24s/it]\n",
      "  1%|▌                                                                             | 120/15663 [00:03<07:12, 35.92it/s]\n",
      "  2%|█▎                                                                            | 264/15663 [00:03<03:20, 76.73it/s]\n",
      "  3%|██                                                                           | 430/15663 [00:03<02:05, 121.42it/s]\n",
      "  3%|██▌                                                                          | 528/15663 [00:03<01:44, 144.53it/s]\n",
      "  5%|███▌                                                                         | 724/15663 [00:03<01:17, 192.88it/s]\n",
      "  6%|████▎                                                                        | 870/15663 [00:03<01:05, 225.75it/s]\n",
      "  7%|█████▏                                                                      | 1068/15663 [00:03<00:54, 270.10it/s]\n",
      "  8%|██████                                                                      | 1238/15663 [00:04<00:47, 304.86it/s]\n",
      "  9%|██████▊                                                                     | 1393/15663 [00:04<00:42, 334.33it/s]\n",
      " 10%|███████▋                                                                    | 1584/15663 [00:04<00:37, 371.19it/s]\n",
      " 11%|████████▍                                                                   | 1747/15663 [00:04<00:34, 398.35it/s]\n",
      " 12%|█████████▎                                                                  | 1927/15663 [00:04<00:31, 429.57it/s]\n",
      " 13%|██████████▏                                                                 | 2090/15663 [00:04<00:29, 455.08it/s]\n",
      " 15%|███████████                                                                 | 2274/15663 [00:04<00:27, 484.57it/s]\n",
      " 16%|███████████▊                                                                | 2444/15663 [00:04<00:25, 509.90it/s]\n",
      " 17%|████████████▋                                                               | 2612/15663 [00:04<00:24, 530.85it/s]\n",
      " 19%|██████████████▏                                                             | 2925/15663 [00:05<00:22, 568.52it/s]Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ME\\NEU\\Anaconda\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\ME\\NEU\\Anaconda\\lib\\site-packages\\tqdm\\_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\ME\\NEU\\Anaconda\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15663/15663 [00:12<00:00, 1304.55it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 3916/3916 [00:02<00:00, 1822.81it/s]\n"
     ]
    }
   ],
   "source": [
    "xtrain_glove = [sent2vec(x) for x in tqdm(X_train)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#changing the taget value to categorical values\n",
    "ytrain_enc = np_utils.to_categorical(y_train)\n",
    "yvalid_enc = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(X_train) + list(X_test))\n",
    "xtrain_seq = token.texts_to_sequences(X_train)\n",
    "xvalid_seq = token.texts_to_sequences(X_test)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 25943/25943 [00:00<00:00, 442393.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building the LSTM RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=True))\n",
    "\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3,return_sequences=True,activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/3\n",
      "15663/15663 [==============================] - ETA: 2:23 - loss: 1.1765 - acc: 0.318 - ETA: 1:52 - loss: 1.2098 - acc: 0.329 - ETA: 1:43 - loss: 1.2374 - acc: 0.339 - ETA: 1:40 - loss: 1.2414 - acc: 0.344 - ETA: 1:36 - loss: 1.2304 - acc: 0.355 - ETA: 1:31 - loss: 1.2231 - acc: 0.353 - ETA: 1:25 - loss: 1.2136 - acc: 0.354 - ETA: 1:22 - loss: 1.2055 - acc: 0.359 - ETA: 1:19 - loss: 1.1974 - acc: 0.360 - ETA: 1:15 - loss: 1.1912 - acc: 0.363 - ETA: 1:11 - loss: 1.1843 - acc: 0.367 - ETA: 1:07 - loss: 1.1806 - acc: 0.366 - ETA: 1:03 - loss: 1.1760 - acc: 0.369 - ETA: 59s - loss: 1.1697 - acc: 0.373 - ETA: 55s - loss: 1.1659 - acc: 0.37 - ETA: 51s - loss: 1.1615 - acc: 0.37 - ETA: 47s - loss: 1.1569 - acc: 0.37 - ETA: 43s - loss: 1.1526 - acc: 0.38 - ETA: 39s - loss: 1.1492 - acc: 0.38 - ETA: 36s - loss: 1.1455 - acc: 0.38 - ETA: 32s - loss: 1.1426 - acc: 0.38 - ETA: 29s - loss: 1.1396 - acc: 0.38 - ETA: 25s - loss: 1.1363 - acc: 0.38 - ETA: 22s - loss: 1.1336 - acc: 0.39 - ETA: 19s - loss: 1.1314 - acc: 0.39 - ETA: 15s - loss: 1.1298 - acc: 0.39 - ETA: 12s - loss: 1.1277 - acc: 0.39 - ETA: 8s - loss: 1.1255 - acc: 0.3954 - ETA: 5s - loss: 1.1234 - acc: 0.396 - ETA: 1s - loss: 1.1217 - acc: 0.397 - 110s 7ms/step - loss: 1.1211 - acc: 0.3977 - val_loss: 1.0646 - val_acc: 0.4798\n",
      "Epoch 2/3\n",
      "15663/15663 [==============================] - ETA: 1:28 - loss: 1.0586 - acc: 0.447 - ETA: 1:26 - loss: 1.0584 - acc: 0.443 - ETA: 1:23 - loss: 1.0620 - acc: 0.436 - ETA: 1:20 - loss: 1.0612 - acc: 0.436 - ETA: 1:17 - loss: 1.0620 - acc: 0.440 - ETA: 1:15 - loss: 1.0570 - acc: 0.445 - ETA: 1:14 - loss: 1.0530 - acc: 0.451 - ETA: 1:12 - loss: 1.0524 - acc: 0.457 - ETA: 1:10 - loss: 1.0497 - acc: 0.458 - ETA: 1:07 - loss: 1.0466 - acc: 0.461 - ETA: 1:03 - loss: 1.0458 - acc: 0.463 - ETA: 59s - loss: 1.0440 - acc: 0.466 - ETA: 55s - loss: 1.0415 - acc: 0.46 - ETA: 52s - loss: 1.0397 - acc: 0.47 - ETA: 49s - loss: 1.0369 - acc: 0.47 - ETA: 45s - loss: 1.0334 - acc: 0.47 - ETA: 42s - loss: 1.0300 - acc: 0.48 - ETA: 39s - loss: 1.0273 - acc: 0.48 - ETA: 35s - loss: 1.0249 - acc: 0.48 - ETA: 32s - loss: 1.0213 - acc: 0.48 - ETA: 29s - loss: 1.0175 - acc: 0.49 - ETA: 26s - loss: 1.0151 - acc: 0.49 - ETA: 23s - loss: 1.0119 - acc: 0.49 - ETA: 20s - loss: 1.0109 - acc: 0.49 - ETA: 17s - loss: 1.0077 - acc: 0.49 - ETA: 14s - loss: 1.0048 - acc: 0.50 - ETA: 11s - loss: 1.0038 - acc: 0.50 - ETA: 8s - loss: 0.9991 - acc: 0.5062 - ETA: 4s - loss: 0.9948 - acc: 0.508 - ETA: 1s - loss: 0.9903 - acc: 0.511 - 104s 7ms/step - loss: 0.9884 - acc: 0.5128 - val_loss: 0.8600 - val_acc: 0.6313\n",
      "Epoch 3/3\n",
      "15663/15663 [==============================] - ETA: 2:05 - loss: 0.8580 - acc: 0.605 - ETA: 1:49 - loss: 0.8480 - acc: 0.624 - ETA: 1:39 - loss: 0.8640 - acc: 0.608 - ETA: 1:32 - loss: 0.8650 - acc: 0.602 - ETA: 1:26 - loss: 0.8560 - acc: 0.613 - ETA: 1:22 - loss: 0.8484 - acc: 0.620 - ETA: 1:17 - loss: 0.8437 - acc: 0.624 - ETA: 1:13 - loss: 0.8307 - acc: 0.632 - ETA: 1:10 - loss: 0.8243 - acc: 0.638 - ETA: 1:06 - loss: 0.8243 - acc: 0.637 - ETA: 1:03 - loss: 0.8214 - acc: 0.640 - ETA: 59s - loss: 0.8219 - acc: 0.641 - ETA: 56s - loss: 0.8205 - acc: 0.64 - ETA: 52s - loss: 0.8173 - acc: 0.64 - ETA: 49s - loss: 0.8127 - acc: 0.64 - ETA: 46s - loss: 0.8102 - acc: 0.65 - ETA: 42s - loss: 0.8052 - acc: 0.65 - ETA: 39s - loss: 0.8067 - acc: 0.65 - ETA: 36s - loss: 0.8017 - acc: 0.65 - ETA: 33s - loss: 0.7973 - acc: 0.65 - ETA: 30s - loss: 0.7932 - acc: 0.66 - ETA: 27s - loss: 0.7913 - acc: 0.66 - ETA: 23s - loss: 0.7880 - acc: 0.66 - ETA: 20s - loss: 0.7846 - acc: 0.66 - ETA: 17s - loss: 0.7794 - acc: 0.66 - ETA: 14s - loss: 0.7746 - acc: 0.66 - ETA: 11s - loss: 0.7714 - acc: 0.67 - ETA: 8s - loss: 0.7683 - acc: 0.6724 - ETA: 4s - loss: 0.7662 - acc: 0.674 - ETA: 1s - loss: 0.7645 - acc: 0.675 - 105s 7ms/step - loss: 0.7645 - acc: 0.6764 - val_loss: 0.6293 - val_acc: 0.7451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23c855e5da0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=3, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 70, 300)           7783200   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 70, 100)           160400    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 7000)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1024)              7169024   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 3075      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 16,165,299\n",
      "Trainable params: 16,165,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores=model.evaluate(xvalid_pad,yvalid_enc,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 75.49%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 58.50%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model.metrics_names[0], scores[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chaning the Activation Function - tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=True))\n",
    "\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3,return_sequences=True,activation=\"tanh\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='tanh'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='tanh'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/3\n",
      "15663/15663 [==============================] - ETA: 3:20 - loss: 1.3116 - acc: 0.353 - ETA: 2:25 - loss: 1.3753 - acc: 0.375 - ETA: 2:05 - loss: 1.5568 - acc: 0.355 - ETA: 1:54 - loss: 1.5133 - acc: 0.369 - ETA: 1:47 - loss: 1.5191 - acc: 0.375 - ETA: 1:40 - loss: 1.4995 - acc: 0.382 - ETA: 1:37 - loss: 1.4898 - acc: 0.380 - ETA: 1:32 - loss: 1.4645 - acc: 0.384 - ETA: 1:27 - loss: 1.4486 - acc: 0.385 - ETA: 1:23 - loss: 1.4331 - acc: 0.389 - ETA: 1:18 - loss: 1.4240 - acc: 0.390 - ETA: 1:14 - loss: 1.4114 - acc: 0.393 - ETA: 1:10 - loss: 1.4056 - acc: 0.394 - ETA: 1:06 - loss: 1.3911 - acc: 0.397 - ETA: 1:02 - loss: 1.3838 - acc: 0.399 - ETA: 58s - loss: 1.3723 - acc: 0.402 - ETA: 54s - loss: 1.3599 - acc: 0.40 - ETA: 50s - loss: 1.3482 - acc: 0.41 - ETA: 46s - loss: 1.3414 - acc: 0.41 - ETA: 42s - loss: 1.3290 - acc: 0.41 - ETA: 38s - loss: 1.3203 - acc: 0.42 - ETA: 34s - loss: 1.3120 - acc: 0.42 - ETA: 29s - loss: 1.3019 - acc: 0.42 - ETA: 25s - loss: 1.2928 - acc: 0.43 - ETA: 21s - loss: 1.2817 - acc: 0.43 - ETA: 17s - loss: 1.2761 - acc: 0.44 - ETA: 13s - loss: 1.2647 - acc: 0.44 - ETA: 10s - loss: 1.2547 - acc: 0.44 - ETA: 6s - loss: 1.2468 - acc: 0.4518 - ETA: 2s - loss: 1.2379 - acc: 0.455 - 125s 8ms/step - loss: 1.2324 - acc: 0.4580 - val_loss: 0.7980 - val_acc: 0.6471\n",
      "Epoch 2/3\n",
      "15663/15663 [==============================] - ETA: 1:41 - loss: 0.9112 - acc: 0.584 - ETA: 1:37 - loss: 0.9415 - acc: 0.567 - ETA: 1:34 - loss: 0.9334 - acc: 0.574 - ETA: 1:31 - loss: 0.9505 - acc: 0.570 - ETA: 1:28 - loss: 0.9533 - acc: 0.566 - ETA: 1:24 - loss: 0.9296 - acc: 0.579 - ETA: 1:21 - loss: 0.9215 - acc: 0.586 - ETA: 1:17 - loss: 0.9187 - acc: 0.587 - ETA: 1:14 - loss: 0.9163 - acc: 0.588 - ETA: 1:11 - loss: 0.9183 - acc: 0.590 - ETA: 1:07 - loss: 0.9136 - acc: 0.590 - ETA: 1:04 - loss: 0.9079 - acc: 0.593 - ETA: 1:01 - loss: 0.9044 - acc: 0.596 - ETA: 57s - loss: 0.9052 - acc: 0.598 - ETA: 54s - loss: 0.9030 - acc: 0.60 - ETA: 50s - loss: 0.9004 - acc: 0.60 - ETA: 47s - loss: 0.8969 - acc: 0.60 - ETA: 43s - loss: 0.8940 - acc: 0.60 - ETA: 40s - loss: 0.8904 - acc: 0.60 - ETA: 36s - loss: 0.8858 - acc: 0.61 - ETA: 33s - loss: 0.8797 - acc: 0.61 - ETA: 30s - loss: 0.8780 - acc: 0.61 - ETA: 26s - loss: 0.8703 - acc: 0.62 - ETA: 23s - loss: 0.8674 - acc: 0.62 - ETA: 19s - loss: 0.8608 - acc: 0.62 - ETA: 16s - loss: 0.8599 - acc: 0.62 - ETA: 12s - loss: 0.8548 - acc: 0.62 - ETA: 9s - loss: 0.8536 - acc: 0.6296 - ETA: 5s - loss: 0.8493 - acc: 0.632 - ETA: 2s - loss: 0.8471 - acc: 0.633 - 116s 7ms/step - loss: 0.8454 - acc: 0.6342 - val_loss: 0.6887 - val_acc: 0.7063\n",
      "Epoch 3/3\n",
      "15663/15663 [==============================] - ETA: 1:42 - loss: 0.6514 - acc: 0.730 - ETA: 1:52 - loss: 0.6948 - acc: 0.705 - ETA: 1:57 - loss: 0.6936 - acc: 0.705 - ETA: 1:59 - loss: 0.7084 - acc: 0.702 - ETA: 1:56 - loss: 0.7178 - acc: 0.701 - ETA: 1:52 - loss: 0.7119 - acc: 0.706 - ETA: 1:45 - loss: 0.7061 - acc: 0.709 - ETA: 1:40 - loss: 0.7029 - acc: 0.711 - ETA: 1:37 - loss: 0.6964 - acc: 0.713 - ETA: 1:32 - loss: 0.6960 - acc: 0.712 - ETA: 1:26 - loss: 0.6951 - acc: 0.714 - ETA: 1:20 - loss: 0.6938 - acc: 0.714 - ETA: 1:15 - loss: 0.6968 - acc: 0.712 - ETA: 1:10 - loss: 0.6899 - acc: 0.716 - ETA: 1:05 - loss: 0.6853 - acc: 0.717 - ETA: 1:00 - loss: 0.6815 - acc: 0.719 - ETA: 55s - loss: 0.6809 - acc: 0.719 - ETA: 51s - loss: 0.6754 - acc: 0.72 - ETA: 46s - loss: 0.6721 - acc: 0.72 - ETA: 42s - loss: 0.6696 - acc: 0.72 - ETA: 38s - loss: 0.6662 - acc: 0.72 - ETA: 34s - loss: 0.6664 - acc: 0.72 - ETA: 29s - loss: 0.6630 - acc: 0.72 - ETA: 25s - loss: 0.6596 - acc: 0.72 - ETA: 21s - loss: 0.6573 - acc: 0.73 - ETA: 17s - loss: 0.6550 - acc: 0.73 - ETA: 14s - loss: 0.6518 - acc: 0.73 - ETA: 10s - loss: 0.6514 - acc: 0.73 - ETA: 6s - loss: 0.6486 - acc: 0.7322 - ETA: 2s - loss: 0.6468 - acc: 0.733 - 127s 8ms/step - loss: 0.6460 - acc: 0.7343 - val_loss: 0.5807 - val_acc: 0.7574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23cc383f668>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=3, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 70, 300)           7783200   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 70, 100)           160400    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 7000)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1024)              7169024   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 3075      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 16,165,299\n",
      "Trainable params: 16,165,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores=model.evaluate(xvalid_pad,yvalid_enc,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 75.74%\n",
      "loss: 58.07%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[0], scores[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the cost function to mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=True))\n",
    "\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3,return_sequences=True,activation=\"tanh\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='tanh'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='tanh'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='mse', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/5\n",
      "15663/15663 [==============================] - ETA: 4:01 - loss: 0.2619 - acc: 0.328 - ETA: 3:07 - loss: 0.2746 - acc: 0.391 - ETA: 2:46 - loss: 0.2862 - acc: 0.375 - ETA: 2:31 - loss: 0.2928 - acc: 0.367 - ETA: 2:21 - loss: 0.2919 - acc: 0.366 - ETA: 2:12 - loss: 0.2903 - acc: 0.376 - ETA: 2:04 - loss: 0.2930 - acc: 0.375 - ETA: 1:56 - loss: 0.2914 - acc: 0.374 - ETA: 1:50 - loss: 0.2921 - acc: 0.372 - ETA: 1:44 - loss: 0.2918 - acc: 0.372 - ETA: 1:38 - loss: 0.2883 - acc: 0.375 - ETA: 1:33 - loss: 0.2868 - acc: 0.377 - ETA: 1:28 - loss: 0.2863 - acc: 0.378 - ETA: 1:22 - loss: 0.2841 - acc: 0.382 - ETA: 1:17 - loss: 0.2825 - acc: 0.385 - ETA: 1:12 - loss: 0.2796 - acc: 0.393 - ETA: 1:07 - loss: 0.2770 - acc: 0.398 - ETA: 1:02 - loss: 0.2753 - acc: 0.401 - ETA: 57s - loss: 0.2729 - acc: 0.405 - ETA: 52s - loss: 0.2704 - acc: 0.41 - ETA: 47s - loss: 0.2682 - acc: 0.41 - ETA: 42s - loss: 0.2667 - acc: 0.41 - ETA: 37s - loss: 0.2648 - acc: 0.42 - ETA: 32s - loss: 0.2630 - acc: 0.42 - ETA: 27s - loss: 0.2609 - acc: 0.43 - ETA: 22s - loss: 0.2594 - acc: 0.43 - ETA: 17s - loss: 0.2575 - acc: 0.44 - ETA: 12s - loss: 0.2556 - acc: 0.44 - ETA: 7s - loss: 0.2538 - acc: 0.4497 - ETA: 2s - loss: 0.2526 - acc: 0.452 - 161s 10ms/step - loss: 0.2516 - acc: 0.4548 - val_loss: 0.1727 - val_acc: 0.6399\n",
      "Epoch 2/5\n",
      "15663/15663 [==============================] - ETA: 2:21 - loss: 0.1995 - acc: 0.576 - ETA: 2:19 - loss: 0.1970 - acc: 0.582 - ETA: 2:13 - loss: 0.1983 - acc: 0.582 - ETA: 2:08 - loss: 0.2009 - acc: 0.575 - ETA: 2:03 - loss: 0.1970 - acc: 0.587 - ETA: 1:59 - loss: 0.1958 - acc: 0.592 - ETA: 1:54 - loss: 0.1945 - acc: 0.595 - ETA: 1:49 - loss: 0.1937 - acc: 0.597 - ETA: 1:45 - loss: 0.1917 - acc: 0.601 - ETA: 1:40 - loss: 0.1893 - acc: 0.606 - ETA: 1:35 - loss: 0.1879 - acc: 0.609 - ETA: 1:30 - loss: 0.1881 - acc: 0.609 - ETA: 1:25 - loss: 0.1870 - acc: 0.612 - ETA: 1:20 - loss: 0.1858 - acc: 0.614 - ETA: 1:15 - loss: 0.1846 - acc: 0.616 - ETA: 1:11 - loss: 0.1838 - acc: 0.617 - ETA: 1:06 - loss: 0.1838 - acc: 0.618 - ETA: 1:01 - loss: 0.1833 - acc: 0.619 - ETA: 56s - loss: 0.1825 - acc: 0.620 - ETA: 51s - loss: 0.1814 - acc: 0.62 - ETA: 46s - loss: 0.1810 - acc: 0.62 - ETA: 41s - loss: 0.1803 - acc: 0.62 - ETA: 36s - loss: 0.1798 - acc: 0.62 - ETA: 31s - loss: 0.1796 - acc: 0.62 - ETA: 27s - loss: 0.1785 - acc: 0.62 - ETA: 22s - loss: 0.1783 - acc: 0.62 - ETA: 17s - loss: 0.1773 - acc: 0.62 - ETA: 12s - loss: 0.1767 - acc: 0.62 - ETA: 7s - loss: 0.1757 - acc: 0.6321 - ETA: 2s - loss: 0.1753 - acc: 0.633 - 158s 10ms/step - loss: 0.1753 - acc: 0.6328 - val_loss: 0.1382 - val_acc: 0.7109\n",
      "Epoch 3/5\n",
      "15663/15663 [==============================] - ETA: 2:22 - loss: 0.1503 - acc: 0.701 - ETA: 2:17 - loss: 0.1508 - acc: 0.690 - ETA: 2:12 - loss: 0.1510 - acc: 0.683 - ETA: 2:06 - loss: 0.1462 - acc: 0.699 - ETA: 2:02 - loss: 0.1477 - acc: 0.693 - ETA: 1:57 - loss: 0.1486 - acc: 0.690 - ETA: 1:53 - loss: 0.1477 - acc: 0.693 - ETA: 1:49 - loss: 0.1454 - acc: 0.697 - ETA: 1:44 - loss: 0.1441 - acc: 0.701 - ETA: 1:39 - loss: 0.1432 - acc: 0.701 - ETA: 1:34 - loss: 0.1423 - acc: 0.704 - ETA: 1:30 - loss: 0.1430 - acc: 0.702 - ETA: 1:25 - loss: 0.1430 - acc: 0.702 - ETA: 1:19 - loss: 0.1423 - acc: 0.704 - ETA: 1:14 - loss: 0.1418 - acc: 0.704 - ETA: 1:09 - loss: 0.1412 - acc: 0.706 - ETA: 1:04 - loss: 0.1409 - acc: 0.706 - ETA: 1:00 - loss: 0.1411 - acc: 0.705 - ETA: 55s - loss: 0.1402 - acc: 0.708 - ETA: 50s - loss: 0.1401 - acc: 0.70 - ETA: 45s - loss: 0.1396 - acc: 0.70 - ETA: 41s - loss: 0.1388 - acc: 0.71 - ETA: 36s - loss: 0.1383 - acc: 0.71 - ETA: 31s - loss: 0.1382 - acc: 0.71 - ETA: 26s - loss: 0.1376 - acc: 0.71 - ETA: 21s - loss: 0.1375 - acc: 0.71 - ETA: 17s - loss: 0.1369 - acc: 0.71 - ETA: 12s - loss: 0.1368 - acc: 0.71 - ETA: 7s - loss: 0.1358 - acc: 0.7171 - ETA: 2s - loss: 0.1353 - acc: 0.718 - 157s 10ms/step - loss: 0.1347 - acc: 0.7193 - val_loss: 0.1196 - val_acc: 0.7497\n",
      "Epoch 4/5\n",
      "15663/15663 [==============================] - ETA: 2:26 - loss: 0.1091 - acc: 0.783 - ETA: 2:19 - loss: 0.1109 - acc: 0.778 - ETA: 2:13 - loss: 0.1081 - acc: 0.782 - ETA: 2:09 - loss: 0.1057 - acc: 0.783 - ETA: 2:03 - loss: 0.1068 - acc: 0.779 - ETA: 1:59 - loss: 0.1058 - acc: 0.784 - ETA: 1:54 - loss: 0.1050 - acc: 0.784 - ETA: 1:48 - loss: 0.1060 - acc: 0.783 - ETA: 1:43 - loss: 0.1070 - acc: 0.781 - ETA: 1:39 - loss: 0.1079 - acc: 0.780 - ETA: 1:34 - loss: 0.1084 - acc: 0.778 - ETA: 1:29 - loss: 0.1090 - acc: 0.777 - ETA: 1:24 - loss: 0.1089 - acc: 0.777 - ETA: 1:19 - loss: 0.1091 - acc: 0.778 - ETA: 1:14 - loss: 0.1092 - acc: 0.778 - ETA: 1:10 - loss: 0.1088 - acc: 0.778 - ETA: 1:05 - loss: 0.1086 - acc: 0.779 - ETA: 1:00 - loss: 0.1080 - acc: 0.779 - ETA: 55s - loss: 0.1080 - acc: 0.779 - ETA: 50s - loss: 0.1079 - acc: 0.77 - ETA: 45s - loss: 0.1078 - acc: 0.77 - ETA: 41s - loss: 0.1073 - acc: 0.78 - ETA: 36s - loss: 0.1067 - acc: 0.78 - ETA: 31s - loss: 0.1064 - acc: 0.78 - ETA: 26s - loss: 0.1061 - acc: 0.78 - ETA: 21s - loss: 0.1058 - acc: 0.78 - ETA: 17s - loss: 0.1053 - acc: 0.78 - ETA: 12s - loss: 0.1050 - acc: 0.78 - ETA: 7s - loss: 0.1049 - acc: 0.7851 - ETA: 2s - loss: 0.1044 - acc: 0.786 - 157s 10ms/step - loss: 0.1045 - acc: 0.7859 - val_loss: 0.1056 - val_acc: 0.7827\n",
      "Epoch 5/5\n",
      "15663/15663 [==============================] - ETA: 2:35 - loss: 0.0892 - acc: 0.824 - ETA: 2:30 - loss: 0.0915 - acc: 0.818 - ETA: 2:21 - loss: 0.0931 - acc: 0.811 - ETA: 2:15 - loss: 0.0878 - acc: 0.822 - ETA: 2:08 - loss: 0.0871 - acc: 0.821 - ETA: 2:03 - loss: 0.0881 - acc: 0.818 - ETA: 1:58 - loss: 0.0866 - acc: 0.821 - ETA: 1:53 - loss: 0.0864 - acc: 0.820 - ETA: 1:48 - loss: 0.0859 - acc: 0.821 - ETA: 1:43 - loss: 0.0863 - acc: 0.820 - ETA: 1:38 - loss: 0.0859 - acc: 0.822 - ETA: 1:33 - loss: 0.0853 - acc: 0.823 - ETA: 1:27 - loss: 0.0847 - acc: 0.824 - ETA: 1:22 - loss: 0.0850 - acc: 0.823 - ETA: 1:17 - loss: 0.0846 - acc: 0.825 - ETA: 1:12 - loss: 0.0850 - acc: 0.824 - ETA: 1:07 - loss: 0.0857 - acc: 0.823 - ETA: 1:02 - loss: 0.0848 - acc: 0.825 - ETA: 57s - loss: 0.0850 - acc: 0.824 - ETA: 52s - loss: 0.0847 - acc: 0.82 - ETA: 47s - loss: 0.0841 - acc: 0.82 - ETA: 42s - loss: 0.0845 - acc: 0.82 - ETA: 37s - loss: 0.0843 - acc: 0.82 - ETA: 32s - loss: 0.0846 - acc: 0.82 - ETA: 27s - loss: 0.0841 - acc: 0.82 - ETA: 22s - loss: 0.0841 - acc: 0.82 - ETA: 17s - loss: 0.0839 - acc: 0.82 - ETA: 12s - loss: 0.0838 - acc: 0.82 - ETA: 7s - loss: 0.0836 - acc: 0.8291 - ETA: 2s - loss: 0.0833 - acc: 0.829 - 161s 10ms/step - loss: 0.0834 - acc: 0.8295 - val_loss: 0.0974 - val_acc: 0.7995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bb5496def0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=5, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores=model.evaluate(xvalid_pad,yvalid_enc,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 79.95%\n",
      "loss: 9.74%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[0], scores[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Epoch changed to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/10\n",
      "15663/15663 [==============================] - ETA: 1:54 - loss: 0.0696 - acc: 0.865 - ETA: 1:46 - loss: 0.0672 - acc: 0.868 - ETA: 1:38 - loss: 0.0662 - acc: 0.865 - ETA: 1:32 - loss: 0.0662 - acc: 0.864 - ETA: 1:28 - loss: 0.0650 - acc: 0.867 - ETA: 1:23 - loss: 0.0670 - acc: 0.861 - ETA: 1:19 - loss: 0.0670 - acc: 0.862 - ETA: 1:15 - loss: 0.0656 - acc: 0.865 - ETA: 1:11 - loss: 0.0639 - acc: 0.868 - ETA: 1:08 - loss: 0.0636 - acc: 0.869 - ETA: 1:05 - loss: 0.0640 - acc: 0.869 - ETA: 1:01 - loss: 0.0634 - acc: 0.870 - ETA: 58s - loss: 0.0645 - acc: 0.868 - ETA: 55s - loss: 0.0651 - acc: 0.86 - ETA: 51s - loss: 0.0648 - acc: 0.86 - ETA: 48s - loss: 0.0644 - acc: 0.86 - ETA: 45s - loss: 0.0644 - acc: 0.87 - ETA: 42s - loss: 0.0651 - acc: 0.86 - ETA: 38s - loss: 0.0649 - acc: 0.86 - ETA: 35s - loss: 0.0647 - acc: 0.86 - ETA: 32s - loss: 0.0647 - acc: 0.86 - ETA: 28s - loss: 0.0647 - acc: 0.86 - ETA: 25s - loss: 0.0650 - acc: 0.86 - ETA: 22s - loss: 0.0654 - acc: 0.86 - ETA: 18s - loss: 0.0655 - acc: 0.86 - ETA: 15s - loss: 0.0652 - acc: 0.86 - ETA: 12s - loss: 0.0652 - acc: 0.86 - ETA: 8s - loss: 0.0651 - acc: 0.8682 - ETA: 5s - loss: 0.0649 - acc: 0.868 - ETA: 1s - loss: 0.0649 - acc: 0.868 - 110s 7ms/step - loss: 0.0648 - acc: 0.8692 - val_loss: 0.0943 - val_acc: 0.8057\n",
      "Epoch 2/10\n",
      "15663/15663 [==============================] - ETA: 1:34 - loss: 0.0563 - acc: 0.892 - ETA: 1:32 - loss: 0.0531 - acc: 0.900 - ETA: 1:29 - loss: 0.0506 - acc: 0.903 - ETA: 1:28 - loss: 0.0494 - acc: 0.905 - ETA: 1:24 - loss: 0.0506 - acc: 0.900 - ETA: 1:23 - loss: 0.0515 - acc: 0.897 - ETA: 1:20 - loss: 0.0510 - acc: 0.898 - ETA: 1:16 - loss: 0.0502 - acc: 0.900 - ETA: 1:12 - loss: 0.0505 - acc: 0.900 - ETA: 1:09 - loss: 0.0501 - acc: 0.900 - ETA: 1:06 - loss: 0.0489 - acc: 0.902 - ETA: 1:03 - loss: 0.0498 - acc: 0.900 - ETA: 1:02 - loss: 0.0499 - acc: 0.901 - ETA: 1:00 - loss: 0.0499 - acc: 0.900 - ETA: 56s - loss: 0.0500 - acc: 0.899 - ETA: 52s - loss: 0.0498 - acc: 0.90 - ETA: 49s - loss: 0.0499 - acc: 0.90 - ETA: 46s - loss: 0.0497 - acc: 0.90 - ETA: 42s - loss: 0.0500 - acc: 0.90 - ETA: 39s - loss: 0.0500 - acc: 0.90 - ETA: 35s - loss: 0.0496 - acc: 0.90 - ETA: 32s - loss: 0.0496 - acc: 0.90 - ETA: 28s - loss: 0.0500 - acc: 0.90 - ETA: 24s - loss: 0.0500 - acc: 0.90 - ETA: 20s - loss: 0.0501 - acc: 0.90 - ETA: 17s - loss: 0.0502 - acc: 0.90 - ETA: 13s - loss: 0.0504 - acc: 0.90 - ETA: 9s - loss: 0.0502 - acc: 0.9001 - ETA: 5s - loss: 0.0505 - acc: 0.899 - ETA: 2s - loss: 0.0502 - acc: 0.900 - 124s 8ms/step - loss: 0.0502 - acc: 0.9003 - val_loss: 0.0945 - val_acc: 0.8126\n",
      "Epoch 3/10\n",
      "15663/15663 [==============================] - ETA: 2:09 - loss: 0.0424 - acc: 0.919 - ETA: 1:51 - loss: 0.0417 - acc: 0.918 - ETA: 1:46 - loss: 0.0420 - acc: 0.917 - ETA: 1:41 - loss: 0.0426 - acc: 0.915 - ETA: 1:38 - loss: 0.0430 - acc: 0.912 - ETA: 1:34 - loss: 0.0438 - acc: 0.911 - ETA: 1:28 - loss: 0.0454 - acc: 0.908 - ETA: 1:23 - loss: 0.0434 - acc: 0.911 - ETA: 1:19 - loss: 0.0439 - acc: 0.910 - ETA: 1:15 - loss: 0.0435 - acc: 0.911 - ETA: 1:11 - loss: 0.0437 - acc: 0.910 - ETA: 1:07 - loss: 0.0435 - acc: 0.911 - ETA: 1:03 - loss: 0.0440 - acc: 0.910 - ETA: 59s - loss: 0.0437 - acc: 0.911 - ETA: 55s - loss: 0.0432 - acc: 0.91 - ETA: 51s - loss: 0.0434 - acc: 0.91 - ETA: 47s - loss: 0.0430 - acc: 0.91 - ETA: 44s - loss: 0.0423 - acc: 0.91 - ETA: 41s - loss: 0.0423 - acc: 0.91 - ETA: 37s - loss: 0.0428 - acc: 0.91 - ETA: 34s - loss: 0.0428 - acc: 0.91 - ETA: 31s - loss: 0.0422 - acc: 0.91 - ETA: 27s - loss: 0.0423 - acc: 0.91 - ETA: 23s - loss: 0.0423 - acc: 0.91 - ETA: 20s - loss: 0.0427 - acc: 0.91 - ETA: 16s - loss: 0.0428 - acc: 0.91 - ETA: 12s - loss: 0.0429 - acc: 0.91 - ETA: 9s - loss: 0.0429 - acc: 0.9141 - ETA: 5s - loss: 0.0428 - acc: 0.914 - ETA: 2s - loss: 0.0426 - acc: 0.914 - 116s 7ms/step - loss: 0.0426 - acc: 0.9151 - val_loss: 0.0919 - val_acc: 0.8179\n",
      "Epoch 4/10\n",
      "15663/15663 [==============================] - ETA: 1:32 - loss: 0.0315 - acc: 0.939 - ETA: 1:31 - loss: 0.0345 - acc: 0.930 - ETA: 1:28 - loss: 0.0345 - acc: 0.931 - ETA: 1:26 - loss: 0.0342 - acc: 0.934 - ETA: 1:22 - loss: 0.0349 - acc: 0.933 - ETA: 1:20 - loss: 0.0348 - acc: 0.931 - ETA: 1:17 - loss: 0.0353 - acc: 0.928 - ETA: 1:14 - loss: 0.0352 - acc: 0.929 - ETA: 1:11 - loss: 0.0371 - acc: 0.925 - ETA: 1:08 - loss: 0.0372 - acc: 0.926 - ETA: 1:06 - loss: 0.0368 - acc: 0.926 - ETA: 1:03 - loss: 0.0370 - acc: 0.926 - ETA: 1:00 - loss: 0.0375 - acc: 0.925 - ETA: 57s - loss: 0.0378 - acc: 0.924 - ETA: 53s - loss: 0.0375 - acc: 0.92 - ETA: 50s - loss: 0.0379 - acc: 0.92 - ETA: 46s - loss: 0.0381 - acc: 0.92 - ETA: 43s - loss: 0.0380 - acc: 0.92 - ETA: 40s - loss: 0.0376 - acc: 0.92 - ETA: 37s - loss: 0.0380 - acc: 0.92 - ETA: 33s - loss: 0.0383 - acc: 0.92 - ETA: 30s - loss: 0.0378 - acc: 0.92 - ETA: 26s - loss: 0.0379 - acc: 0.92 - ETA: 23s - loss: 0.0378 - acc: 0.92 - ETA: 19s - loss: 0.0376 - acc: 0.92 - ETA: 16s - loss: 0.0373 - acc: 0.92 - ETA: 12s - loss: 0.0369 - acc: 0.92 - ETA: 9s - loss: 0.0369 - acc: 0.9260 - ETA: 5s - loss: 0.0368 - acc: 0.926 - ETA: 2s - loss: 0.0366 - acc: 0.926 - 114s 7ms/step - loss: 0.0367 - acc: 0.9263 - val_loss: 0.0919 - val_acc: 0.8184\n",
      "Epoch 5/10\n",
      "15663/15663 [==============================] - ETA: 1:37 - loss: 0.0208 - acc: 0.959 - ETA: 1:33 - loss: 0.0273 - acc: 0.949 - ETA: 1:29 - loss: 0.0286 - acc: 0.943 - ETA: 1:26 - loss: 0.0287 - acc: 0.943 - ETA: 1:23 - loss: 0.0302 - acc: 0.939 - ETA: 1:20 - loss: 0.0302 - acc: 0.941 - ETA: 1:18 - loss: 0.0296 - acc: 0.940 - ETA: 1:14 - loss: 0.0291 - acc: 0.941 - ETA: 1:11 - loss: 0.0297 - acc: 0.940 - ETA: 1:08 - loss: 0.0299 - acc: 0.940 - ETA: 1:05 - loss: 0.0299 - acc: 0.941 - ETA: 1:01 - loss: 0.0295 - acc: 0.941 - ETA: 58s - loss: 0.0289 - acc: 0.942 - ETA: 55s - loss: 0.0292 - acc: 0.94 - ETA: 51s - loss: 0.0292 - acc: 0.94 - ETA: 48s - loss: 0.0292 - acc: 0.94 - ETA: 45s - loss: 0.0287 - acc: 0.94 - ETA: 42s - loss: 0.0287 - acc: 0.94 - ETA: 38s - loss: 0.0285 - acc: 0.94 - ETA: 35s - loss: 0.0290 - acc: 0.94 - ETA: 31s - loss: 0.0288 - acc: 0.94 - ETA: 28s - loss: 0.0288 - acc: 0.94 - ETA: 25s - loss: 0.0287 - acc: 0.94 - ETA: 21s - loss: 0.0286 - acc: 0.94 - ETA: 18s - loss: 0.0285 - acc: 0.94 - ETA: 15s - loss: 0.0287 - acc: 0.94 - ETA: 11s - loss: 0.0288 - acc: 0.94 - ETA: 8s - loss: 0.0289 - acc: 0.9432 - ETA: 5s - loss: 0.0285 - acc: 0.943 - ETA: 1s - loss: 0.0286 - acc: 0.943 - 110s 7ms/step - loss: 0.0286 - acc: 0.9437 - val_loss: 0.0917 - val_acc: 0.8212\n",
      "Epoch 6/10\n",
      "15663/15663 [==============================] - ETA: 1:42 - loss: 0.0284 - acc: 0.949 - ETA: 1:38 - loss: 0.0247 - acc: 0.953 - ETA: 1:33 - loss: 0.0233 - acc: 0.955 - ETA: 1:29 - loss: 0.0238 - acc: 0.954 - ETA: 1:25 - loss: 0.0247 - acc: 0.955 - ETA: 1:22 - loss: 0.0245 - acc: 0.954 - ETA: 1:19 - loss: 0.0236 - acc: 0.956 - ETA: 1:16 - loss: 0.0242 - acc: 0.955 - ETA: 1:14 - loss: 0.0231 - acc: 0.957 - ETA: 1:12 - loss: 0.0231 - acc: 0.957 - ETA: 1:08 - loss: 0.0228 - acc: 0.957 - ETA: 1:05 - loss: 0.0227 - acc: 0.957 - ETA: 1:01 - loss: 0.0225 - acc: 0.957 - ETA: 57s - loss: 0.0230 - acc: 0.956 - ETA: 54s - loss: 0.0233 - acc: 0.95 - ETA: 50s - loss: 0.0232 - acc: 0.95 - ETA: 46s - loss: 0.0228 - acc: 0.95 - ETA: 43s - loss: 0.0227 - acc: 0.95 - ETA: 39s - loss: 0.0229 - acc: 0.95 - ETA: 36s - loss: 0.0228 - acc: 0.95 - ETA: 32s - loss: 0.0232 - acc: 0.95 - ETA: 29s - loss: 0.0233 - acc: 0.95 - ETA: 25s - loss: 0.0231 - acc: 0.95 - ETA: 22s - loss: 0.0231 - acc: 0.95 - ETA: 18s - loss: 0.0229 - acc: 0.95 - ETA: 15s - loss: 0.0230 - acc: 0.95 - ETA: 12s - loss: 0.0231 - acc: 0.95 - ETA: 8s - loss: 0.0230 - acc: 0.9563 - ETA: 5s - loss: 0.0228 - acc: 0.956 - ETA: 2s - loss: 0.0227 - acc: 0.957 - 117s 7ms/step - loss: 0.0228 - acc: 0.9569 - val_loss: 0.0929 - val_acc: 0.8220\n",
      "Epoch 7/10\n",
      "15663/15663 [==============================] - ETA: 2:12 - loss: 0.0175 - acc: 0.966 - ETA: 2:18 - loss: 0.0175 - acc: 0.968 - ETA: 2:10 - loss: 0.0181 - acc: 0.967 - ETA: 2:01 - loss: 0.0197 - acc: 0.962 - ETA: 1:55 - loss: 0.0202 - acc: 0.961 - ETA: 1:48 - loss: 0.0202 - acc: 0.961 - ETA: 1:42 - loss: 0.0198 - acc: 0.962 - ETA: 1:37 - loss: 0.0197 - acc: 0.963 - ETA: 1:31 - loss: 0.0191 - acc: 0.964 - ETA: 1:26 - loss: 0.0192 - acc: 0.964 - ETA: 1:21 - loss: 0.0200 - acc: 0.962 - ETA: 1:16 - loss: 0.0200 - acc: 0.962 - ETA: 1:12 - loss: 0.0199 - acc: 0.962 - ETA: 1:07 - loss: 0.0200 - acc: 0.962 - ETA: 1:03 - loss: 0.0200 - acc: 0.962 - ETA: 59s - loss: 0.0197 - acc: 0.962 - ETA: 54s - loss: 0.0199 - acc: 0.96 - ETA: 50s - loss: 0.0199 - acc: 0.96 - ETA: 46s - loss: 0.0198 - acc: 0.96 - ETA: 42s - loss: 0.0198 - acc: 0.96 - ETA: 38s - loss: 0.0200 - acc: 0.96 - ETA: 34s - loss: 0.0198 - acc: 0.96 - ETA: 30s - loss: 0.0201 - acc: 0.96 - ETA: 26s - loss: 0.0201 - acc: 0.96 - ETA: 22s - loss: 0.0200 - acc: 0.96 - ETA: 18s - loss: 0.0200 - acc: 0.96 - ETA: 14s - loss: 0.0200 - acc: 0.96 - ETA: 10s - loss: 0.0200 - acc: 0.96 - ETA: 6s - loss: 0.0202 - acc: 0.9617 - ETA: 2s - loss: 0.0200 - acc: 0.962 - 129s 8ms/step - loss: 0.0199 - acc: 0.9625 - val_loss: 0.0924 - val_acc: 0.8281\n",
      "Epoch 8/10\n",
      "15663/15663 [==============================] - ETA: 1:49 - loss: 0.0162 - acc: 0.972 - ETA: 1:45 - loss: 0.0162 - acc: 0.971 - ETA: 1:41 - loss: 0.0190 - acc: 0.964 - ETA: 1:37 - loss: 0.0195 - acc: 0.964 - ETA: 1:32 - loss: 0.0196 - acc: 0.963 - ETA: 1:29 - loss: 0.0187 - acc: 0.964 - ETA: 1:26 - loss: 0.0190 - acc: 0.963 - ETA: 1:22 - loss: 0.0184 - acc: 0.964 - ETA: 1:18 - loss: 0.0184 - acc: 0.964 - ETA: 1:15 - loss: 0.0180 - acc: 0.965 - ETA: 1:11 - loss: 0.0180 - acc: 0.965 - ETA: 1:07 - loss: 0.0179 - acc: 0.965 - ETA: 1:03 - loss: 0.0182 - acc: 0.965 - ETA: 1:00 - loss: 0.0184 - acc: 0.965 - ETA: 57s - loss: 0.0181 - acc: 0.965 - ETA: 53s - loss: 0.0185 - acc: 0.96 - ETA: 49s - loss: 0.0185 - acc: 0.96 - ETA: 45s - loss: 0.0184 - acc: 0.96 - ETA: 42s - loss: 0.0185 - acc: 0.96 - ETA: 38s - loss: 0.0185 - acc: 0.96 - ETA: 34s - loss: 0.0190 - acc: 0.96 - ETA: 31s - loss: 0.0193 - acc: 0.96 - ETA: 27s - loss: 0.0192 - acc: 0.96 - ETA: 23s - loss: 0.0190 - acc: 0.96 - ETA: 20s - loss: 0.0190 - acc: 0.96 - ETA: 16s - loss: 0.0190 - acc: 0.96 - ETA: 13s - loss: 0.0191 - acc: 0.96 - ETA: 9s - loss: 0.0189 - acc: 0.9646 - ETA: 5s - loss: 0.0188 - acc: 0.964 - ETA: 2s - loss: 0.0188 - acc: 0.964 - 119s 8ms/step - loss: 0.0186 - acc: 0.9649 - val_loss: 0.0925 - val_acc: 0.8279\n",
      "Epoch 9/10\n",
      "15663/15663 [==============================] - ETA: 1:46 - loss: 0.0147 - acc: 0.972 - ETA: 1:45 - loss: 0.0141 - acc: 0.974 - ETA: 1:42 - loss: 0.0172 - acc: 0.968 - ETA: 1:39 - loss: 0.0164 - acc: 0.968 - ETA: 1:35 - loss: 0.0157 - acc: 0.969 - ETA: 1:31 - loss: 0.0155 - acc: 0.970 - ETA: 1:28 - loss: 0.0157 - acc: 0.969 - ETA: 1:24 - loss: 0.0154 - acc: 0.970 - ETA: 1:20 - loss: 0.0158 - acc: 0.969 - ETA: 1:17 - loss: 0.0163 - acc: 0.968 - ETA: 1:13 - loss: 0.0162 - acc: 0.969 - ETA: 1:09 - loss: 0.0157 - acc: 0.970 - ETA: 1:05 - loss: 0.0158 - acc: 0.970 - ETA: 1:01 - loss: 0.0160 - acc: 0.969 - ETA: 57s - loss: 0.0164 - acc: 0.969 - ETA: 53s - loss: 0.0166 - acc: 0.96 - ETA: 50s - loss: 0.0169 - acc: 0.96 - ETA: 46s - loss: 0.0168 - acc: 0.96 - ETA: 42s - loss: 0.0166 - acc: 0.96 - ETA: 39s - loss: 0.0166 - acc: 0.96 - ETA: 35s - loss: 0.0164 - acc: 0.96 - ETA: 32s - loss: 0.0164 - acc: 0.96 - ETA: 28s - loss: 0.0161 - acc: 0.97 - ETA: 24s - loss: 0.0163 - acc: 0.96 - ETA: 20s - loss: 0.0162 - acc: 0.97 - ETA: 17s - loss: 0.0160 - acc: 0.97 - ETA: 13s - loss: 0.0158 - acc: 0.97 - ETA: 9s - loss: 0.0157 - acc: 0.9708 - ETA: 5s - loss: 0.0157 - acc: 0.970 - ETA: 2s - loss: 0.0158 - acc: 0.970 - 121s 8ms/step - loss: 0.0158 - acc: 0.9703 - val_loss: 0.0940 - val_acc: 0.8269\n",
      "Epoch 10/10\n",
      "15663/15663 [==============================] - ETA: 1:44 - loss: 0.0117 - acc: 0.978 - ETA: 1:45 - loss: 0.0119 - acc: 0.979 - ETA: 1:42 - loss: 0.0125 - acc: 0.979 - ETA: 1:38 - loss: 0.0123 - acc: 0.979 - ETA: 1:34 - loss: 0.0132 - acc: 0.978 - ETA: 1:30 - loss: 0.0139 - acc: 0.976 - ETA: 1:26 - loss: 0.0145 - acc: 0.975 - ETA: 1:23 - loss: 0.0143 - acc: 0.974 - ETA: 1:19 - loss: 0.0143 - acc: 0.973 - ETA: 1:15 - loss: 0.0138 - acc: 0.974 - ETA: 1:11 - loss: 0.0133 - acc: 0.975 - ETA: 1:08 - loss: 0.0135 - acc: 0.975 - ETA: 1:04 - loss: 0.0135 - acc: 0.975 - ETA: 1:01 - loss: 0.0137 - acc: 0.974 - ETA: 57s - loss: 0.0135 - acc: 0.975 - ETA: 53s - loss: 0.0131 - acc: 0.97 - ETA: 49s - loss: 0.0128 - acc: 0.97 - ETA: 46s - loss: 0.0127 - acc: 0.97 - ETA: 42s - loss: 0.0128 - acc: 0.97 - ETA: 39s - loss: 0.0128 - acc: 0.97 - ETA: 35s - loss: 0.0128 - acc: 0.97 - ETA: 31s - loss: 0.0125 - acc: 0.97 - ETA: 28s - loss: 0.0125 - acc: 0.97 - ETA: 24s - loss: 0.0129 - acc: 0.97 - ETA: 20s - loss: 0.0131 - acc: 0.97 - ETA: 16s - loss: 0.0131 - acc: 0.97 - ETA: 13s - loss: 0.0132 - acc: 0.97 - ETA: 9s - loss: 0.0131 - acc: 0.9754 - ETA: 5s - loss: 0.0133 - acc: 0.975 - ETA: 2s - loss: 0.0133 - acc: 0.975 - 122s 8ms/step - loss: 0.0135 - acc: 0.9748 - val_loss: 0.0961 - val_acc: 0.8243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bb90b81b38>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=10, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores=model.evaluate(xvalid_pad,yvalid_enc,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 82.43%\n",
      "loss: 9.61%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[0], scores[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gradient estimation  - Changed From adam to RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=True))\n",
    "\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3,return_sequences=True,activation=\"tanh\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='tanh'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='tanh'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/3\n",
      "15663/15663 [==============================] - ETA: 3:13 - loss: 1.3121 - acc: 0.324 - ETA: 2:27 - loss: 1.6723 - acc: 0.356 - ETA: 2:08 - loss: 3.5516 - acc: 0.337 - ETA: 1:57 - loss: 4.2387 - acc: 0.351 - ETA: 1:48 - loss: 4.2799 - acc: 0.351 - ETA: 1:41 - loss: 4.0141 - acc: 0.352 - ETA: 1:36 - loss: 3.6446 - acc: 0.355 - ETA: 1:31 - loss: 3.3756 - acc: 0.357 - ETA: 1:26 - loss: 3.1502 - acc: 0.360 - ETA: 1:21 - loss: 2.9788 - acc: 0.363 - ETA: 1:17 - loss: 2.8360 - acc: 0.362 - ETA: 1:13 - loss: 2.7196 - acc: 0.367 - ETA: 1:09 - loss: 2.6011 - acc: 0.371 - ETA: 1:04 - loss: 2.5042 - acc: 0.374 - ETA: 1:00 - loss: 2.4218 - acc: 0.378 - ETA: 56s - loss: 2.3459 - acc: 0.384 - ETA: 52s - loss: 2.2759 - acc: 0.38 - ETA: 48s - loss: 2.2100 - acc: 0.39 - ETA: 44s - loss: 2.1505 - acc: 0.40 - ETA: 40s - loss: 2.1023 - acc: 0.40 - ETA: 36s - loss: 2.0568 - acc: 0.40 - ETA: 32s - loss: 2.0110 - acc: 0.41 - ETA: 28s - loss: 1.9714 - acc: 0.41 - ETA: 25s - loss: 1.9347 - acc: 0.42 - ETA: 21s - loss: 1.9059 - acc: 0.42 - ETA: 17s - loss: 1.8774 - acc: 0.42 - ETA: 13s - loss: 1.8462 - acc: 0.42 - ETA: 9s - loss: 1.8150 - acc: 0.4324 - ETA: 6s - loss: 1.7873 - acc: 0.436 - ETA: 2s - loss: 1.7647 - acc: 0.437 - 124s 8ms/step - loss: 1.7525 - acc: 0.4387 - val_loss: 0.8813 - val_acc: 0.5861\n",
      "Epoch 2/3\n",
      "15663/15663 [==============================] - ETA: 1:45 - loss: 0.9710 - acc: 0.566 - ETA: 1:42 - loss: 0.9680 - acc: 0.577 - ETA: 1:39 - loss: 0.9861 - acc: 0.563 - ETA: 1:35 - loss: 0.9932 - acc: 0.557 - ETA: 1:32 - loss: 0.9994 - acc: 0.551 - ETA: 1:28 - loss: 0.9859 - acc: 0.558 - ETA: 1:25 - loss: 0.9861 - acc: 0.564 - ETA: 1:22 - loss: 0.9798 - acc: 0.565 - ETA: 1:18 - loss: 0.9969 - acc: 0.559 - ETA: 1:14 - loss: 1.0101 - acc: 0.554 - ETA: 1:10 - loss: 1.0106 - acc: 0.548 - ETA: 1:06 - loss: 1.0127 - acc: 0.544 - ETA: 1:03 - loss: 1.0037 - acc: 0.548 - ETA: 59s - loss: 1.0004 - acc: 0.549 - ETA: 56s - loss: 0.9973 - acc: 0.55 - ETA: 52s - loss: 0.9931 - acc: 0.55 - ETA: 49s - loss: 0.9857 - acc: 0.55 - ETA: 45s - loss: 0.9819 - acc: 0.56 - ETA: 42s - loss: 0.9780 - acc: 0.56 - ETA: 38s - loss: 0.9767 - acc: 0.56 - ETA: 34s - loss: 0.9698 - acc: 0.56 - ETA: 31s - loss: 0.9647 - acc: 0.57 - ETA: 27s - loss: 0.9599 - acc: 0.57 - ETA: 24s - loss: 0.9573 - acc: 0.57 - ETA: 20s - loss: 0.9495 - acc: 0.57 - ETA: 17s - loss: 0.9453 - acc: 0.58 - ETA: 13s - loss: 0.9408 - acc: 0.58 - ETA: 9s - loss: 0.9383 - acc: 0.5851 - ETA: 6s - loss: 0.9351 - acc: 0.586 - ETA: 2s - loss: 0.9338 - acc: 0.588 - 126s 8ms/step - loss: 0.9309 - acc: 0.5894 - val_loss: 0.7044 - val_acc: 0.6897\n",
      "Epoch 3/3\n",
      "15663/15663 [==============================] - ETA: 2:00 - loss: 0.7620 - acc: 0.669 - ETA: 1:57 - loss: 0.8156 - acc: 0.658 - ETA: 1:59 - loss: 0.8300 - acc: 0.652 - ETA: 1:52 - loss: 0.8202 - acc: 0.653 - ETA: 1:48 - loss: 0.8201 - acc: 0.655 - ETA: 1:43 - loss: 0.8162 - acc: 0.657 - ETA: 1:38 - loss: 0.8082 - acc: 0.663 - ETA: 1:33 - loss: 0.8033 - acc: 0.663 - ETA: 1:29 - loss: 0.7979 - acc: 0.665 - ETA: 1:25 - loss: 0.7871 - acc: 0.671 - ETA: 1:21 - loss: 0.7773 - acc: 0.676 - ETA: 1:16 - loss: 0.7738 - acc: 0.677 - ETA: 1:12 - loss: 0.7707 - acc: 0.678 - ETA: 1:08 - loss: 0.7746 - acc: 0.676 - ETA: 1:04 - loss: 0.7796 - acc: 0.673 - ETA: 1:00 - loss: 0.7767 - acc: 0.672 - ETA: 56s - loss: 0.7820 - acc: 0.669 - ETA: 52s - loss: 0.7812 - acc: 0.66 - ETA: 48s - loss: 0.7803 - acc: 0.66 - ETA: 44s - loss: 0.7767 - acc: 0.67 - ETA: 40s - loss: 0.7748 - acc: 0.67 - ETA: 36s - loss: 0.7707 - acc: 0.67 - ETA: 32s - loss: 0.7697 - acc: 0.67 - ETA: 28s - loss: 0.7700 - acc: 0.67 - ETA: 23s - loss: 0.7707 - acc: 0.67 - ETA: 19s - loss: 0.7736 - acc: 0.67 - ETA: 15s - loss: 0.7793 - acc: 0.67 - ETA: 11s - loss: 0.7777 - acc: 0.67 - ETA: 6s - loss: 0.7743 - acc: 0.6729 - ETA: 2s - loss: 0.7698 - acc: 0.675 - 145s 9ms/step - loss: 0.7680 - acc: 0.6764 - val_loss: 0.6590 - val_acc: 0.7257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bba02f6d68>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=3, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores=model.evaluate(xvalid_pad,yvalid_enc,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 72.57%\n",
      "loss: 65.90%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[0], scores[0]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Network Architecture - Changing the number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=True))\n",
    "\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3,activation=\"tanh\"))\n",
    "\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(1024, activation='tanh'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/3\n",
      "15663/15663 [==============================] - ETA: 3:06 - loss: 1.2254 - acc: 0.287 - ETA: 2:11 - loss: 1.1902 - acc: 0.320 - ETA: 1:50 - loss: 1.1679 - acc: 0.343 - ETA: 1:38 - loss: 1.1532 - acc: 0.353 - ETA: 1:29 - loss: 1.1495 - acc: 0.359 - ETA: 1:23 - loss: 1.1458 - acc: 0.368 - ETA: 1:19 - loss: 1.1456 - acc: 0.368 - ETA: 1:14 - loss: 1.1437 - acc: 0.368 - ETA: 1:10 - loss: 1.1397 - acc: 0.372 - ETA: 1:06 - loss: 1.1353 - acc: 0.375 - ETA: 1:03 - loss: 1.1338 - acc: 0.375 - ETA: 59s - loss: 1.1322 - acc: 0.376 - ETA: 56s - loss: 1.1294 - acc: 0.37 - ETA: 52s - loss: 1.1288 - acc: 0.37 - ETA: 49s - loss: 1.1280 - acc: 0.37 - ETA: 46s - loss: 1.1255 - acc: 0.37 - ETA: 43s - loss: 1.1234 - acc: 0.37 - ETA: 39s - loss: 1.1226 - acc: 0.37 - ETA: 36s - loss: 1.1214 - acc: 0.37 - ETA: 33s - loss: 1.1206 - acc: 0.37 - ETA: 30s - loss: 1.1192 - acc: 0.37 - ETA: 26s - loss: 1.1167 - acc: 0.38 - ETA: 23s - loss: 1.1158 - acc: 0.38 - ETA: 20s - loss: 1.1136 - acc: 0.38 - ETA: 17s - loss: 1.1115 - acc: 0.38 - ETA: 14s - loss: 1.1106 - acc: 0.38 - ETA: 11s - loss: 1.1093 - acc: 0.38 - ETA: 8s - loss: 1.1081 - acc: 0.3915 - ETA: 4s - loss: 1.1063 - acc: 0.392 - ETA: 1s - loss: 1.1050 - acc: 0.394 - 101s 6ms/step - loss: 1.1032 - acc: 0.3963 - val_loss: 1.0041 - val_acc: 0.4990\n",
      "Epoch 2/3\n",
      "15663/15663 [==============================] - ETA: 1:29 - loss: 1.0762 - acc: 0.425 - ETA: 1:28 - loss: 1.0727 - acc: 0.434 - ETA: 1:25 - loss: 1.0630 - acc: 0.449 - ETA: 1:21 - loss: 1.0619 - acc: 0.450 - ETA: 1:17 - loss: 1.0561 - acc: 0.452 - ETA: 1:14 - loss: 1.0532 - acc: 0.454 - ETA: 1:11 - loss: 1.0516 - acc: 0.459 - ETA: 1:07 - loss: 1.0505 - acc: 0.460 - ETA: 1:04 - loss: 1.0488 - acc: 0.462 - ETA: 1:01 - loss: 1.0463 - acc: 0.463 - ETA: 57s - loss: 1.0448 - acc: 0.465 - ETA: 55s - loss: 1.0425 - acc: 0.46 - ETA: 52s - loss: 1.0402 - acc: 0.46 - ETA: 50s - loss: 1.0381 - acc: 0.47 - ETA: 48s - loss: 1.0347 - acc: 0.47 - ETA: 45s - loss: 1.0336 - acc: 0.47 - ETA: 42s - loss: 1.0309 - acc: 0.47 - ETA: 40s - loss: 1.0290 - acc: 0.47 - ETA: 37s - loss: 1.0275 - acc: 0.48 - ETA: 34s - loss: 1.0255 - acc: 0.48 - ETA: 31s - loss: 1.0237 - acc: 0.48 - ETA: 27s - loss: 1.0200 - acc: 0.48 - ETA: 24s - loss: 1.0185 - acc: 0.48 - ETA: 21s - loss: 1.0159 - acc: 0.48 - ETA: 18s - loss: 1.0138 - acc: 0.49 - ETA: 14s - loss: 1.0109 - acc: 0.49 - ETA: 11s - loss: 1.0085 - acc: 0.49 - ETA: 8s - loss: 1.0068 - acc: 0.4974 - ETA: 5s - loss: 1.0029 - acc: 0.500 - ETA: 1s - loss: 0.9993 - acc: 0.503 - 107s 7ms/step - loss: 0.9980 - acc: 0.5044 - val_loss: 0.8301 - val_acc: 0.6274\n",
      "Epoch 3/3\n",
      "15663/15663 [==============================] - ETA: 1:36 - loss: 0.9052 - acc: 0.595 - ETA: 1:35 - loss: 0.8939 - acc: 0.596 - ETA: 1:35 - loss: 0.8813 - acc: 0.592 - ETA: 1:34 - loss: 0.8896 - acc: 0.588 - ETA: 1:33 - loss: 0.8928 - acc: 0.584 - ETA: 1:27 - loss: 0.8954 - acc: 0.585 - ETA: 1:21 - loss: 0.8923 - acc: 0.583 - ETA: 1:15 - loss: 0.8877 - acc: 0.589 - ETA: 1:11 - loss: 0.8854 - acc: 0.591 - ETA: 1:06 - loss: 0.8782 - acc: 0.597 - ETA: 1:02 - loss: 0.8803 - acc: 0.597 - ETA: 58s - loss: 0.8805 - acc: 0.596 - ETA: 54s - loss: 0.8783 - acc: 0.59 - ETA: 51s - loss: 0.8839 - acc: 0.59 - ETA: 47s - loss: 0.8812 - acc: 0.59 - ETA: 44s - loss: 0.8798 - acc: 0.59 - ETA: 41s - loss: 0.8774 - acc: 0.59 - ETA: 37s - loss: 0.8766 - acc: 0.60 - ETA: 34s - loss: 0.8755 - acc: 0.60 - ETA: 31s - loss: 0.8736 - acc: 0.60 - ETA: 28s - loss: 0.8697 - acc: 0.60 - ETA: 25s - loss: 0.8693 - acc: 0.60 - ETA: 22s - loss: 0.8681 - acc: 0.61 - ETA: 19s - loss: 0.8653 - acc: 0.61 - ETA: 16s - loss: 0.8622 - acc: 0.61 - ETA: 13s - loss: 0.8612 - acc: 0.61 - ETA: 10s - loss: 0.8598 - acc: 0.61 - ETA: 7s - loss: 0.8592 - acc: 0.6172 - ETA: 4s - loss: 0.8562 - acc: 0.619 - ETA: 1s - loss: 0.8550 - acc: 0.621 - 95s 6ms/step - loss: 0.8542 - acc: 0.6216 - val_loss: 0.7037 - val_acc: 0.6999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bb9f011828>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=3, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores=model.evaluate(xvalid_pad,yvalid_enc,verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 69.99%\n",
      "loss: 70.37%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[0], scores[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Network initialization -Changed to  uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=True))\n",
    "\n",
    "model.add(LSTM(100,kernel_initializer=\"uniform\", dropout=0.3, recurrent_dropout=0.3,activation=\"tanh\"))\n",
    "\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(1024, kernel_initializer=\"uniform\",activation='tanh'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/3\n",
      "15663/15663 [==============================] - ETA: 2:43 - loss: 1.1029 - acc: 0.345 - ETA: 1:57 - loss: 1.1004 - acc: 0.364 - ETA: 1:40 - loss: 1.0961 - acc: 0.372 - ETA: 1:30 - loss: 1.0943 - acc: 0.382 - ETA: 1:23 - loss: 1.0925 - acc: 0.385 - ETA: 1:18 - loss: 1.0893 - acc: 0.393 - ETA: 1:13 - loss: 1.0940 - acc: 0.388 - ETA: 1:09 - loss: 1.0925 - acc: 0.393 - ETA: 1:05 - loss: 1.0913 - acc: 0.396 - ETA: 1:01 - loss: 1.0891 - acc: 0.400 - ETA: 58s - loss: 1.0885 - acc: 0.400 - ETA: 54s - loss: 1.0874 - acc: 0.40 - ETA: 51s - loss: 1.0863 - acc: 0.40 - ETA: 48s - loss: 1.0850 - acc: 0.40 - ETA: 45s - loss: 1.0837 - acc: 0.40 - ETA: 42s - loss: 1.0815 - acc: 0.41 - ETA: 39s - loss: 1.0813 - acc: 0.41 - ETA: 36s - loss: 1.0794 - acc: 0.41 - ETA: 33s - loss: 1.0775 - acc: 0.41 - ETA: 30s - loss: 1.0752 - acc: 0.41 - ETA: 27s - loss: 1.0737 - acc: 0.41 - ETA: 24s - loss: 1.0722 - acc: 0.42 - ETA: 21s - loss: 1.0712 - acc: 0.42 - ETA: 18s - loss: 1.0709 - acc: 0.42 - ETA: 15s - loss: 1.0687 - acc: 0.42 - ETA: 13s - loss: 1.0657 - acc: 0.42 - ETA: 10s - loss: 1.0635 - acc: 0.43 - ETA: 7s - loss: 1.0617 - acc: 0.4344 - ETA: 4s - loss: 1.0591 - acc: 0.438 - ETA: 1s - loss: 1.0565 - acc: 0.441 - 94s 6ms/step - loss: 1.0550 - acc: 0.4431 - val_loss: 0.9154 - val_acc: 0.5802\n",
      "Epoch 2/3\n",
      "15663/15663 [==============================] - ETA: 1:26 - loss: 0.9151 - acc: 0.566 - ETA: 1:20 - loss: 0.9567 - acc: 0.553 - ETA: 1:16 - loss: 0.9546 - acc: 0.549 - ETA: 1:13 - loss: 0.9516 - acc: 0.551 - ETA: 1:11 - loss: 0.9485 - acc: 0.555 - ETA: 1:08 - loss: 0.9506 - acc: 0.551 - ETA: 1:05 - loss: 0.9484 - acc: 0.553 - ETA: 1:02 - loss: 0.9427 - acc: 0.558 - ETA: 59s - loss: 0.9398 - acc: 0.559 - ETA: 57s - loss: 0.9372 - acc: 0.56 - ETA: 54s - loss: 0.9330 - acc: 0.56 - ETA: 51s - loss: 0.9287 - acc: 0.56 - ETA: 48s - loss: 0.9271 - acc: 0.57 - ETA: 46s - loss: 0.9244 - acc: 0.57 - ETA: 43s - loss: 0.9238 - acc: 0.57 - ETA: 40s - loss: 0.9195 - acc: 0.57 - ETA: 37s - loss: 0.9148 - acc: 0.57 - ETA: 35s - loss: 0.9154 - acc: 0.57 - ETA: 32s - loss: 0.9135 - acc: 0.58 - ETA: 29s - loss: 0.9123 - acc: 0.58 - ETA: 26s - loss: 0.9099 - acc: 0.58 - ETA: 24s - loss: 0.9089 - acc: 0.58 - ETA: 21s - loss: 0.9063 - acc: 0.58 - ETA: 18s - loss: 0.9038 - acc: 0.58 - ETA: 15s - loss: 0.8994 - acc: 0.58 - ETA: 12s - loss: 0.8951 - acc: 0.59 - ETA: 10s - loss: 0.8933 - acc: 0.59 - ETA: 7s - loss: 0.8926 - acc: 0.5951 - ETA: 4s - loss: 0.8912 - acc: 0.596 - ETA: 1s - loss: 0.8888 - acc: 0.597 - 92s 6ms/step - loss: 0.8882 - acc: 0.5983 - val_loss: 0.7392 - val_acc: 0.6816\n",
      "Epoch 3/3\n",
      "15663/15663 [==============================] - ETA: 1:23 - loss: 0.7879 - acc: 0.666 - ETA: 1:19 - loss: 0.8016 - acc: 0.655 - ETA: 1:16 - loss: 0.7733 - acc: 0.677 - ETA: 1:13 - loss: 0.7744 - acc: 0.678 - ETA: 1:10 - loss: 0.7819 - acc: 0.673 - ETA: 1:08 - loss: 0.7791 - acc: 0.675 - ETA: 1:05 - loss: 0.7753 - acc: 0.677 - ETA: 1:02 - loss: 0.7718 - acc: 0.677 - ETA: 1:00 - loss: 0.7811 - acc: 0.676 - ETA: 57s - loss: 0.7761 - acc: 0.677 - ETA: 55s - loss: 0.7718 - acc: 0.67 - ETA: 53s - loss: 0.7705 - acc: 0.68 - ETA: 50s - loss: 0.7649 - acc: 0.68 - ETA: 47s - loss: 0.7631 - acc: 0.68 - ETA: 44s - loss: 0.7619 - acc: 0.68 - ETA: 41s - loss: 0.7569 - acc: 0.68 - ETA: 38s - loss: 0.7562 - acc: 0.68 - ETA: 35s - loss: 0.7517 - acc: 0.68 - ETA: 33s - loss: 0.7536 - acc: 0.68 - ETA: 30s - loss: 0.7520 - acc: 0.68 - ETA: 27s - loss: 0.7500 - acc: 0.69 - ETA: 24s - loss: 0.7483 - acc: 0.69 - ETA: 21s - loss: 0.7440 - acc: 0.69 - ETA: 18s - loss: 0.7418 - acc: 0.69 - ETA: 15s - loss: 0.7383 - acc: 0.69 - ETA: 13s - loss: 0.7385 - acc: 0.69 - ETA: 10s - loss: 0.7338 - acc: 0.70 - ETA: 7s - loss: 0.7301 - acc: 0.7031 - ETA: 4s - loss: 0.7281 - acc: 0.703 - ETA: 1s - loss: 0.7255 - acc: 0.705 - 94s 6ms/step - loss: 0.7249 - acc: 0.7058 - val_loss: 0.5998 - val_acc: 0.7543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bbd1fef630>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=3, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores=model.evaluate(xvalid_pad,yvalid_enc,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 75.43%\n",
      "loss: 59.98%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[0], scores[0]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
