{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"train\\train.csv\")\n",
    "test = pd.read_csv(r\"test\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "lb= LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, ..., 0, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train.text.values\n",
    "y = lb.fit_transform(train.author.values)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51644it [00:05, 8993.20it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-46b36014d681>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mcoefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0membeddings_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoefs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ME\\NEU\\Anaconda\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \"\"\"\n\u001b[1;32m--> 531\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '.'"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.840B.300d.txt','r',encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method isalpha() checks whether the string consists of alphabetic characters only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    words = [lemm.lemmatize(w) for w in words]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/15663 [00:00<?, ?it/s]\n",
      "  0%|                                                                             | 1/15663 [00:02<12:14:03,  2.81s/it]\n",
      "  1%|▋                                                                             | 137/15663 [00:02<05:30, 47.04it/s]\n",
      "  2%|█▋                                                                           | 331/15663 [00:03<02:20, 109.31it/s]\n",
      "  3%|██▌                                                                          | 516/15663 [00:03<01:31, 164.94it/s]\n",
      "  4%|███▍                                                                         | 695/15663 [00:03<01:09, 215.25it/s]\n",
      "  6%|████▎                                                                        | 876/15663 [00:03<00:56, 263.16it/s]\n",
      "  7%|█████▏                                                                      | 1080/15663 [00:03<00:46, 313.56it/s]\n",
      "  8%|██████▎                                                                     | 1296/15663 [00:03<00:39, 364.07it/s]\n",
      " 10%|███████▎                                                                    | 1501/15663 [00:03<00:34, 410.12it/s]\n",
      " 11%|████████▎                                                                   | 1706/15663 [00:03<00:30, 453.72it/s]\n",
      " 12%|█████████▍                                                                  | 1943/15663 [00:03<00:27, 501.34it/s]\n",
      " 14%|██████████▎                                                                 | 2135/15663 [00:03<00:25, 536.98it/s]\n",
      " 15%|███████████▎                                                                | 2327/15663 [00:04<00:23, 570.19it/s]\n",
      " 16%|████████████▏                                                               | 2516/15663 [00:04<00:21, 601.75it/s]\n",
      " 17%|█████████████▏                                                              | 2730/15663 [00:04<00:20, 635.39it/s]\n",
      " 19%|██████████████▏                                                             | 2929/15663 [00:04<00:19, 666.07it/s]\n",
      " 20%|███████████████▏                                                            | 3121/15663 [00:04<00:18, 693.85it/s]\n",
      " 21%|████████████████                                                            | 3312/15663 [00:04<00:17, 720.29it/s]\n",
      " 22%|█████████████████                                                           | 3513/15663 [00:04<00:16, 745.28it/s]\n",
      " 24%|██████████████████                                                          | 3715/15663 [00:04<00:15, 769.32it/s]\n",
      " 25%|██████████████████▉                                                         | 3898/15663 [00:04<00:14, 790.77it/s]Exception in thread Thread-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ME\\NEU\\Anaconda\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\ME\\NEU\\Anaconda\\lib\\site-packages\\tqdm\\_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\ME\\NEU\\Anaconda\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15663/15663 [00:10<00:00, 1470.04it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 3916/3916 [00:01<00:00, 2163.66it/s]\n"
     ]
    }
   ],
   "source": [
    "xtrain_glove = [sent2vec(x) for x in tqdm(X_train)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ytrain_enc = np_utils.to_categorical(y_train)\n",
    "yvalid_enc = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(300, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/5\n",
      "15663/15663 [==============================] - ETA: 4:38 - loss: 1.1585 - acc: 0.312 - ETA: 1:12 - loss: 1.1325 - acc: 0.394 - ETA: 33s - loss: 1.1274 - acc: 0.380 - ETA: 23s - loss: 1.1183 - acc: 0.39 - ETA: 18s - loss: 1.1233 - acc: 0.39 - ETA: 14s - loss: 1.1078 - acc: 0.42 - ETA: 12s - loss: 1.0881 - acc: 0.43 - ETA: 10s - loss: 1.0709 - acc: 0.45 - ETA: 9s - loss: 1.0678 - acc: 0.4566 - ETA: 8s - loss: 1.0523 - acc: 0.468 - ETA: 8s - loss: 1.0376 - acc: 0.481 - ETA: 7s - loss: 1.0160 - acc: 0.496 - ETA: 6s - loss: 1.0035 - acc: 0.507 - ETA: 6s - loss: 0.9992 - acc: 0.509 - ETA: 5s - loss: 0.9882 - acc: 0.519 - ETA: 5s - loss: 0.9803 - acc: 0.524 - ETA: 5s - loss: 0.9740 - acc: 0.527 - ETA: 5s - loss: 0.9728 - acc: 0.529 - ETA: 5s - loss: 0.9702 - acc: 0.530 - ETA: 5s - loss: 0.9665 - acc: 0.533 - ETA: 5s - loss: 0.9628 - acc: 0.535 - ETA: 5s - loss: 0.9634 - acc: 0.534 - ETA: 4s - loss: 0.9608 - acc: 0.536 - ETA: 4s - loss: 0.9602 - acc: 0.539 - ETA: 4s - loss: 0.9545 - acc: 0.543 - ETA: 4s - loss: 0.9537 - acc: 0.544 - ETA: 4s - loss: 0.9518 - acc: 0.546 - ETA: 4s - loss: 0.9483 - acc: 0.548 - ETA: 4s - loss: 0.9461 - acc: 0.549 - ETA: 4s - loss: 0.9425 - acc: 0.550 - ETA: 4s - loss: 0.9392 - acc: 0.553 - ETA: 3s - loss: 0.9388 - acc: 0.554 - ETA: 3s - loss: 0.9369 - acc: 0.555 - ETA: 3s - loss: 0.9339 - acc: 0.557 - ETA: 3s - loss: 0.9312 - acc: 0.559 - ETA: 3s - loss: 0.9270 - acc: 0.561 - ETA: 3s - loss: 0.9246 - acc: 0.563 - ETA: 3s - loss: 0.9240 - acc: 0.564 - ETA: 3s - loss: 0.9206 - acc: 0.566 - ETA: 2s - loss: 0.9170 - acc: 0.569 - ETA: 2s - loss: 0.9130 - acc: 0.571 - ETA: 2s - loss: 0.9105 - acc: 0.572 - ETA: 2s - loss: 0.9074 - acc: 0.575 - ETA: 2s - loss: 0.9025 - acc: 0.577 - ETA: 2s - loss: 0.9018 - acc: 0.578 - ETA: 2s - loss: 0.8999 - acc: 0.580 - ETA: 1s - loss: 0.8960 - acc: 0.583 - ETA: 1s - loss: 0.8958 - acc: 0.583 - ETA: 1s - loss: 0.8920 - acc: 0.585 - ETA: 1s - loss: 0.8901 - acc: 0.586 - ETA: 1s - loss: 0.8907 - acc: 0.586 - ETA: 1s - loss: 0.8896 - acc: 0.587 - ETA: 1s - loss: 0.8873 - acc: 0.589 - ETA: 1s - loss: 0.8857 - acc: 0.590 - ETA: 1s - loss: 0.8842 - acc: 0.591 - ETA: 0s - loss: 0.8820 - acc: 0.593 - ETA: 0s - loss: 0.8794 - acc: 0.595 - ETA: 0s - loss: 0.8775 - acc: 0.596 - ETA: 0s - loss: 0.8757 - acc: 0.596 - ETA: 0s - loss: 0.8738 - acc: 0.598 - ETA: 0s - loss: 0.8731 - acc: 0.599 - ETA: 0s - loss: 0.8717 - acc: 0.600 - ETA: 0s - loss: 0.8687 - acc: 0.601 - ETA: 0s - loss: 0.8676 - acc: 0.603 - ETA: 0s - loss: 0.8667 - acc: 0.604 - ETA: 0s - loss: 0.8656 - acc: 0.605 - 6s 353us/step - loss: 0.8639 - acc: 0.6062 - val_loss: 0.7621 - val_acc: 0.6729\n",
      "Epoch 2/5\n",
      "15663/15663 [==============================] - ETA: 3s - loss: 0.8620 - acc: 0.609 - ETA: 4s - loss: 0.7379 - acc: 0.668 - ETA: 5s - loss: 0.7987 - acc: 0.643 - ETA: 5s - loss: 0.7939 - acc: 0.654 - ETA: 5s - loss: 0.7723 - acc: 0.653 - ETA: 4s - loss: 0.7742 - acc: 0.650 - ETA: 4s - loss: 0.7741 - acc: 0.649 - ETA: 4s - loss: 0.7738 - acc: 0.656 - ETA: 3s - loss: 0.7654 - acc: 0.661 - ETA: 3s - loss: 0.7705 - acc: 0.661 - ETA: 3s - loss: 0.7669 - acc: 0.665 - ETA: 3s - loss: 0.7684 - acc: 0.664 - ETA: 3s - loss: 0.7634 - acc: 0.666 - ETA: 3s - loss: 0.7609 - acc: 0.667 - ETA: 3s - loss: 0.7570 - acc: 0.671 - ETA: 3s - loss: 0.7583 - acc: 0.670 - ETA: 2s - loss: 0.7623 - acc: 0.671 - ETA: 2s - loss: 0.7640 - acc: 0.668 - ETA: 2s - loss: 0.7627 - acc: 0.667 - ETA: 2s - loss: 0.7610 - acc: 0.669 - ETA: 2s - loss: 0.7683 - acc: 0.665 - ETA: 2s - loss: 0.7679 - acc: 0.666 - ETA: 2s - loss: 0.7696 - acc: 0.664 - ETA: 2s - loss: 0.7680 - acc: 0.664 - ETA: 2s - loss: 0.7690 - acc: 0.664 - ETA: 2s - loss: 0.7690 - acc: 0.663 - ETA: 2s - loss: 0.7699 - acc: 0.662 - ETA: 2s - loss: 0.7701 - acc: 0.662 - ETA: 2s - loss: 0.7669 - acc: 0.663 - ETA: 2s - loss: 0.7662 - acc: 0.663 - ETA: 2s - loss: 0.7648 - acc: 0.664 - ETA: 1s - loss: 0.7667 - acc: 0.664 - ETA: 1s - loss: 0.7625 - acc: 0.667 - ETA: 1s - loss: 0.7632 - acc: 0.666 - ETA: 1s - loss: 0.7615 - acc: 0.667 - ETA: 1s - loss: 0.7622 - acc: 0.668 - ETA: 1s - loss: 0.7616 - acc: 0.669 - ETA: 1s - loss: 0.7613 - acc: 0.669 - ETA: 1s - loss: 0.7606 - acc: 0.670 - ETA: 1s - loss: 0.7608 - acc: 0.670 - ETA: 1s - loss: 0.7617 - acc: 0.669 - ETA: 1s - loss: 0.7614 - acc: 0.670 - ETA: 1s - loss: 0.7631 - acc: 0.668 - ETA: 1s - loss: 0.7637 - acc: 0.668 - ETA: 1s - loss: 0.7639 - acc: 0.668 - ETA: 1s - loss: 0.7636 - acc: 0.668 - ETA: 1s - loss: 0.7628 - acc: 0.668 - ETA: 0s - loss: 0.7633 - acc: 0.668 - ETA: 0s - loss: 0.7628 - acc: 0.668 - ETA: 0s - loss: 0.7643 - acc: 0.667 - ETA: 0s - loss: 0.7637 - acc: 0.667 - ETA: 0s - loss: 0.7629 - acc: 0.669 - ETA: 0s - loss: 0.7621 - acc: 0.669 - ETA: 0s - loss: 0.7616 - acc: 0.669 - ETA: 0s - loss: 0.7618 - acc: 0.669 - ETA: 0s - loss: 0.7615 - acc: 0.669 - ETA: 0s - loss: 0.7601 - acc: 0.670 - ETA: 0s - loss: 0.7596 - acc: 0.669 - ETA: 0s - loss: 0.7596 - acc: 0.670 - ETA: 0s - loss: 0.7603 - acc: 0.669 - ETA: 0s - loss: 0.7598 - acc: 0.670 - ETA: 0s - loss: 0.7592 - acc: 0.670 - ETA: 0s - loss: 0.7600 - acc: 0.670 - ETA: 0s - loss: 0.7589 - acc: 0.671 - 4s 268us/step - loss: 0.7598 - acc: 0.6715 - val_loss: 0.7434 - val_acc: 0.6851\n",
      "Epoch 3/5\n",
      "15663/15663 [==============================] - ETA: 3s - loss: 0.7180 - acc: 0.656 - ETA: 3s - loss: 0.7697 - acc: 0.643 - ETA: 4s - loss: 0.7957 - acc: 0.638 - ETA: 4s - loss: 0.7772 - acc: 0.646 - ETA: 5s - loss: 0.7781 - acc: 0.657 - ETA: 4s - loss: 0.7648 - acc: 0.665 - ETA: 4s - loss: 0.7564 - acc: 0.668 - ETA: 4s - loss: 0.7554 - acc: 0.672 - ETA: 4s - loss: 0.7443 - acc: 0.675 - ETA: 3s - loss: 0.7456 - acc: 0.673 - ETA: 3s - loss: 0.7409 - acc: 0.677 - ETA: 3s - loss: 0.7406 - acc: 0.680 - ETA: 3s - loss: 0.7319 - acc: 0.685 - ETA: 3s - loss: 0.7350 - acc: 0.685 - ETA: 3s - loss: 0.7284 - acc: 0.688 - ETA: 3s - loss: 0.7291 - acc: 0.688 - ETA: 3s - loss: 0.7303 - acc: 0.688 - ETA: 3s - loss: 0.7292 - acc: 0.688 - ETA: 3s - loss: 0.7290 - acc: 0.688 - ETA: 3s - loss: 0.7368 - acc: 0.685 - ETA: 3s - loss: 0.7367 - acc: 0.685 - ETA: 3s - loss: 0.7356 - acc: 0.686 - ETA: 3s - loss: 0.7381 - acc: 0.686 - ETA: 3s - loss: 0.7392 - acc: 0.685 - ETA: 2s - loss: 0.7380 - acc: 0.686 - ETA: 2s - loss: 0.7358 - acc: 0.687 - ETA: 2s - loss: 0.7342 - acc: 0.689 - ETA: 2s - loss: 0.7352 - acc: 0.687 - ETA: 2s - loss: 0.7337 - acc: 0.688 - ETA: 2s - loss: 0.7351 - acc: 0.688 - ETA: 2s - loss: 0.7372 - acc: 0.687 - ETA: 2s - loss: 0.7365 - acc: 0.687 - ETA: 2s - loss: 0.7365 - acc: 0.687 - ETA: 2s - loss: 0.7365 - acc: 0.687 - ETA: 2s - loss: 0.7360 - acc: 0.686 - ETA: 2s - loss: 0.7394 - acc: 0.684 - ETA: 2s - loss: 0.7387 - acc: 0.684 - ETA: 2s - loss: 0.7393 - acc: 0.683 - ETA: 2s - loss: 0.7384 - acc: 0.683 - ETA: 2s - loss: 0.7396 - acc: 0.682 - ETA: 2s - loss: 0.7380 - acc: 0.683 - ETA: 2s - loss: 0.7381 - acc: 0.683 - ETA: 1s - loss: 0.7363 - acc: 0.683 - ETA: 1s - loss: 0.7360 - acc: 0.683 - ETA: 1s - loss: 0.7365 - acc: 0.684 - ETA: 1s - loss: 0.7356 - acc: 0.684 - ETA: 1s - loss: 0.7339 - acc: 0.685 - ETA: 1s - loss: 0.7332 - acc: 0.686 - ETA: 1s - loss: 0.7330 - acc: 0.686 - ETA: 1s - loss: 0.7335 - acc: 0.686 - ETA: 1s - loss: 0.7328 - acc: 0.686 - ETA: 1s - loss: 0.7339 - acc: 0.686 - ETA: 1s - loss: 0.7349 - acc: 0.686 - ETA: 1s - loss: 0.7364 - acc: 0.685 - ETA: 1s - loss: 0.7359 - acc: 0.684 - ETA: 1s - loss: 0.7360 - acc: 0.684 - ETA: 1s - loss: 0.7356 - acc: 0.684 - ETA: 0s - loss: 0.7347 - acc: 0.684 - ETA: 0s - loss: 0.7344 - acc: 0.684 - ETA: 0s - loss: 0.7341 - acc: 0.684 - ETA: 0s - loss: 0.7336 - acc: 0.684 - ETA: 0s - loss: 0.7335 - acc: 0.684 - ETA: 0s - loss: 0.7336 - acc: 0.684 - ETA: 0s - loss: 0.7336 - acc: 0.684 - ETA: 0s - loss: 0.7341 - acc: 0.684 - ETA: 0s - loss: 0.7334 - acc: 0.684 - ETA: 0s - loss: 0.7334 - acc: 0.684 - ETA: 0s - loss: 0.7334 - acc: 0.684 - ETA: 0s - loss: 0.7328 - acc: 0.684 - ETA: 0s - loss: 0.7315 - acc: 0.685 - 5s 296us/step - loss: 0.7316 - acc: 0.6851 - val_loss: 0.7280 - val_acc: 0.6828\n",
      "Epoch 4/5\n",
      "15663/15663 [==============================] - ETA: 3s - loss: 0.6316 - acc: 0.734 - ETA: 3s - loss: 0.7247 - acc: 0.687 - ETA: 4s - loss: 0.7204 - acc: 0.683 - ETA: 4s - loss: 0.7467 - acc: 0.664 - ETA: 3s - loss: 0.7658 - acc: 0.664 - ETA: 3s - loss: 0.7550 - acc: 0.666 - ETA: 3s - loss: 0.7382 - acc: 0.682 - ETA: 3s - loss: 0.7360 - acc: 0.686 - ETA: 2s - loss: 0.7182 - acc: 0.694 - ETA: 2s - loss: 0.7207 - acc: 0.693 - ETA: 2s - loss: 0.7147 - acc: 0.697 - ETA: 2s - loss: 0.7233 - acc: 0.692 - ETA: 2s - loss: 0.7243 - acc: 0.690 - ETA: 2s - loss: 0.7229 - acc: 0.690 - ETA: 2s - loss: 0.7292 - acc: 0.683 - ETA: 2s - loss: 0.7284 - acc: 0.685 - ETA: 2s - loss: 0.7284 - acc: 0.685 - ETA: 2s - loss: 0.7290 - acc: 0.684 - ETA: 2s - loss: 0.7280 - acc: 0.687 - ETA: 1s - loss: 0.7295 - acc: 0.686 - ETA: 1s - loss: 0.7319 - acc: 0.685 - ETA: 1s - loss: 0.7298 - acc: 0.686 - ETA: 1s - loss: 0.7309 - acc: 0.686 - ETA: 1s - loss: 0.7308 - acc: 0.685 - ETA: 1s - loss: 0.7276 - acc: 0.687 - ETA: 1s - loss: 0.7254 - acc: 0.687 - ETA: 1s - loss: 0.7246 - acc: 0.686 - ETA: 1s - loss: 0.7261 - acc: 0.686 - ETA: 1s - loss: 0.7241 - acc: 0.687 - ETA: 1s - loss: 0.7230 - acc: 0.687 - ETA: 1s - loss: 0.7226 - acc: 0.688 - ETA: 1s - loss: 0.7232 - acc: 0.687 - ETA: 1s - loss: 0.7242 - acc: 0.688 - ETA: 1s - loss: 0.7258 - acc: 0.687 - ETA: 1s - loss: 0.7238 - acc: 0.688 - ETA: 1s - loss: 0.7244 - acc: 0.687 - ETA: 0s - loss: 0.7242 - acc: 0.687 - ETA: 0s - loss: 0.7230 - acc: 0.688 - ETA: 0s - loss: 0.7226 - acc: 0.688 - ETA: 0s - loss: 0.7213 - acc: 0.689 - ETA: 0s - loss: 0.7210 - acc: 0.689 - ETA: 0s - loss: 0.7207 - acc: 0.689 - ETA: 0s - loss: 0.7199 - acc: 0.690 - ETA: 0s - loss: 0.7200 - acc: 0.689 - ETA: 0s - loss: 0.7178 - acc: 0.691 - ETA: 0s - loss: 0.7174 - acc: 0.691 - ETA: 0s - loss: 0.7160 - acc: 0.692 - ETA: 0s - loss: 0.7160 - acc: 0.692 - ETA: 0s - loss: 0.7160 - acc: 0.692 - ETA: 0s - loss: 0.7151 - acc: 0.693 - ETA: 0s - loss: 0.7170 - acc: 0.692 - ETA: 0s - loss: 0.7166 - acc: 0.692 - ETA: 0s - loss: 0.7166 - acc: 0.692 - ETA: 0s - loss: 0.7159 - acc: 0.692 - 3s 223us/step - loss: 0.7160 - acc: 0.6923 - val_loss: 0.7247 - val_acc: 0.6887\n",
      "Epoch 5/5\n",
      "15663/15663 [==============================] - ETA: 3s - loss: 0.6698 - acc: 0.734 - ETA: 4s - loss: 0.7491 - acc: 0.695 - ETA: 5s - loss: 0.7175 - acc: 0.710 - ETA: 4s - loss: 0.7593 - acc: 0.673 - ETA: 4s - loss: 0.7563 - acc: 0.679 - ETA: 4s - loss: 0.7481 - acc: 0.683 - ETA: 3s - loss: 0.7399 - acc: 0.684 - ETA: 3s - loss: 0.7293 - acc: 0.686 - ETA: 3s - loss: 0.7270 - acc: 0.686 - ETA: 3s - loss: 0.7206 - acc: 0.691 - ETA: 3s - loss: 0.7221 - acc: 0.690 - ETA: 3s - loss: 0.7132 - acc: 0.698 - ETA: 3s - loss: 0.7192 - acc: 0.693 - ETA: 2s - loss: 0.7158 - acc: 0.696 - ETA: 2s - loss: 0.7135 - acc: 0.696 - ETA: 2s - loss: 0.7176 - acc: 0.693 - ETA: 2s - loss: 0.7105 - acc: 0.698 - ETA: 2s - loss: 0.7103 - acc: 0.698 - ETA: 2s - loss: 0.7098 - acc: 0.700 - ETA: 2s - loss: 0.7096 - acc: 0.700 - ETA: 2s - loss: 0.7099 - acc: 0.699 - ETA: 2s - loss: 0.7074 - acc: 0.700 - ETA: 2s - loss: 0.7101 - acc: 0.697 - ETA: 2s - loss: 0.7106 - acc: 0.697 - ETA: 2s - loss: 0.7114 - acc: 0.697 - ETA: 2s - loss: 0.7079 - acc: 0.699 - ETA: 2s - loss: 0.7090 - acc: 0.699 - ETA: 2s - loss: 0.7107 - acc: 0.698 - ETA: 2s - loss: 0.7099 - acc: 0.698 - ETA: 2s - loss: 0.7092 - acc: 0.699 - ETA: 2s - loss: 0.7076 - acc: 0.701 - ETA: 2s - loss: 0.7055 - acc: 0.701 - ETA: 2s - loss: 0.7084 - acc: 0.699 - ETA: 2s - loss: 0.7045 - acc: 0.701 - ETA: 2s - loss: 0.7026 - acc: 0.702 - ETA: 2s - loss: 0.7007 - acc: 0.702 - ETA: 2s - loss: 0.7000 - acc: 0.702 - ETA: 2s - loss: 0.7009 - acc: 0.701 - ETA: 2s - loss: 0.7023 - acc: 0.701 - ETA: 2s - loss: 0.7017 - acc: 0.701 - ETA: 2s - loss: 0.7004 - acc: 0.702 - ETA: 2s - loss: 0.6988 - acc: 0.703 - ETA: 1s - loss: 0.7001 - acc: 0.703 - ETA: 1s - loss: 0.7017 - acc: 0.702 - ETA: 1s - loss: 0.7004 - acc: 0.702 - ETA: 1s - loss: 0.7003 - acc: 0.702 - ETA: 1s - loss: 0.7007 - acc: 0.702 - ETA: 1s - loss: 0.7017 - acc: 0.701 - ETA: 1s - loss: 0.7017 - acc: 0.701 - ETA: 1s - loss: 0.7020 - acc: 0.701 - ETA: 1s - loss: 0.7022 - acc: 0.700 - ETA: 1s - loss: 0.7015 - acc: 0.701 - ETA: 1s - loss: 0.7006 - acc: 0.701 - ETA: 1s - loss: 0.7000 - acc: 0.701 - ETA: 1s - loss: 0.7001 - acc: 0.701 - ETA: 0s - loss: 0.6984 - acc: 0.703 - ETA: 0s - loss: 0.6970 - acc: 0.704 - ETA: 0s - loss: 0.6974 - acc: 0.703 - ETA: 0s - loss: 0.6976 - acc: 0.703 - ETA: 0s - loss: 0.6985 - acc: 0.702 - ETA: 0s - loss: 0.6975 - acc: 0.702 - ETA: 0s - loss: 0.6981 - acc: 0.701 - ETA: 0s - loss: 0.6994 - acc: 0.700 - ETA: 0s - loss: 0.6991 - acc: 0.700 - ETA: 0s - loss: 0.6992 - acc: 0.700 - 4s 271us/step - loss: 0.6992 - acc: 0.7010 - val_loss: 0.7197 - val_acc: 0.6928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b57455f828>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=5, verbose=1, \n",
    "          validation_data=(xvalid_glove_scl, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(X_train) + list(X_test))\n",
    "xtrain_seq = token.texts_to_sequences(X_train)\n",
    "xvalid_seq = token.texts_to_sequences(X_test)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 25943/25943 [00:00<00:00, 345508.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/2\n",
      "15663/15663 [==============================] - ETA: 1:44 - loss: 1.1294 - acc: 0.335 - ETA: 1:16 - loss: 1.1239 - acc: 0.340 - ETA: 1:05 - loss: 1.1155 - acc: 0.360 - ETA: 58s - loss: 1.1178 - acc: 0.362 - ETA: 54s - loss: 1.1166 - acc: 0.37 - ETA: 51s - loss: 1.1155 - acc: 0.37 - ETA: 48s - loss: 1.1116 - acc: 0.37 - ETA: 45s - loss: 1.1103 - acc: 0.37 - ETA: 43s - loss: 1.1086 - acc: 0.38 - ETA: 41s - loss: 1.1077 - acc: 0.37 - ETA: 39s - loss: 1.1060 - acc: 0.37 - ETA: 37s - loss: 1.1058 - acc: 0.37 - ETA: 35s - loss: 1.1062 - acc: 0.37 - ETA: 33s - loss: 1.1047 - acc: 0.37 - ETA: 31s - loss: 1.1045 - acc: 0.37 - ETA: 29s - loss: 1.1035 - acc: 0.37 - ETA: 27s - loss: 1.1030 - acc: 0.37 - ETA: 25s - loss: 1.1015 - acc: 0.37 - ETA: 23s - loss: 1.1003 - acc: 0.38 - ETA: 21s - loss: 1.0999 - acc: 0.38 - ETA: 20s - loss: 1.0988 - acc: 0.38 - ETA: 17s - loss: 1.0975 - acc: 0.38 - ETA: 15s - loss: 1.0964 - acc: 0.38 - ETA: 13s - loss: 1.0952 - acc: 0.38 - ETA: 11s - loss: 1.0936 - acc: 0.38 - ETA: 9s - loss: 1.0926 - acc: 0.3878 - ETA: 7s - loss: 1.0916 - acc: 0.389 - ETA: 5s - loss: 1.0904 - acc: 0.391 - ETA: 3s - loss: 1.0894 - acc: 0.392 - ETA: 1s - loss: 1.0881 - acc: 0.394 - 69s 4ms/step - loss: 1.0872 - acc: 0.3953 - val_loss: 1.0164 - val_acc: 0.4939\n",
      "Epoch 2/2\n",
      "15663/15663 [==============================] - ETA: 57s - loss: 1.0325 - acc: 0.46 - ETA: 55s - loss: 1.0462 - acc: 0.45 - ETA: 53s - loss: 1.0407 - acc: 0.46 - ETA: 51s - loss: 1.0399 - acc: 0.46 - ETA: 49s - loss: 1.0409 - acc: 0.46 - ETA: 47s - loss: 1.0398 - acc: 0.46 - ETA: 45s - loss: 1.0421 - acc: 0.46 - ETA: 43s - loss: 1.0360 - acc: 0.46 - ETA: 42s - loss: 1.0356 - acc: 0.46 - ETA: 40s - loss: 1.0337 - acc: 0.47 - ETA: 38s - loss: 1.0287 - acc: 0.47 - ETA: 36s - loss: 1.0266 - acc: 0.47 - ETA: 34s - loss: 1.0236 - acc: 0.47 - ETA: 32s - loss: 1.0210 - acc: 0.48 - ETA: 30s - loss: 1.0207 - acc: 0.48 - ETA: 28s - loss: 1.0178 - acc: 0.48 - ETA: 26s - loss: 1.0170 - acc: 0.48 - ETA: 24s - loss: 1.0163 - acc: 0.48 - ETA: 22s - loss: 1.0138 - acc: 0.48 - ETA: 20s - loss: 1.0133 - acc: 0.48 - ETA: 19s - loss: 1.0129 - acc: 0.48 - ETA: 17s - loss: 1.0133 - acc: 0.48 - ETA: 15s - loss: 1.0120 - acc: 0.49 - ETA: 13s - loss: 1.0115 - acc: 0.49 - ETA: 11s - loss: 1.0103 - acc: 0.49 - ETA: 9s - loss: 1.0075 - acc: 0.4952 - ETA: 7s - loss: 1.0069 - acc: 0.495 - ETA: 5s - loss: 1.0048 - acc: 0.497 - ETA: 3s - loss: 1.0040 - acc: 0.497 - ETA: 1s - loss: 1.0039 - acc: 0.496 - 67s 4ms/step - loss: 1.0027 - acc: 0.4974 - val_loss: 0.9003 - val_acc: 0.5848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b543dfb710>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=2, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/nikbearbrown/NEU_COE/blob/master/Deep_Learning/RNNs/7.1%20RNN%20and%20LSTM.ipynb\n",
    "seq = Sequential()\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   input_shape=(None, 40, 40, 1),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n",
    "               activation='sigmoid',\n",
    "               padding='same', data_format='channels_last'))\n",
    "seq.compile(loss='binary_crossentropy', optimizer='adadelta')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
